---
title: "Association-Based Spectral Clustering for Mixed Data"
subtitle: "Intermediate Workshop of the PRIN 2022 project, 5-6 February 2026, Bologna"
author: "Alfonso Iodice D'Enza, Cristina Tortora and Francesco Palumbo"
# date: "22 June, 2023"
title-slide-attributes:
  data-background-color: "#E6C65C"
  data-background-image: figures/wp_bol_logo.png
  data-background-position: bottom
  data-background-size: contain
  data-background-opacity: "0.75"
  data-color: "#4A3F1F"
format: 
  revealjs: 
    html-math-method:
      method: mathjax
      url: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js
    # smaller: true
    # logo: figures/wp_bol_logo.png
    theme: [default, title_slide.scss]
    fontsize: 1.5em
    preview-links: true
    transition: fade
    # bibliography: spectral_mix_biblio.bib
    # csl: asa.csl
    progress: true
    slide-number: true
    embed-resources: true
---


```{r, warning=FALSE,message=FALSE}
library(FactoMineR)
library(GGally)
library("gganimate")
library(ggrepel)
library(magick)
library(palmerpenguins)
library("tidyverse")
library("fpc")
library("factoextra")
library("dbscan")
library("mclust")
library(fastDummies)
library(mlbench)
library(tidyverse)
library(tidymodels)
library(purrr)
library(ca)
library("magick")
library(tidymodels)
library(discrim)
library(clustrd)
library(flipbookr)
library(kableExtra)
library(janitor)
library(patchwork)
library(mvtnorm)
library("ggraph")
library("tidygraph")
library("GGally")
library("tidyverse")
library("network")
library("sna")
library("ggplot2")
library("kableExtra")
library("RefManageR")

BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
       cite.style = "authoryear",
       style = "markdown",
       hyperlink = FALSE,
       dashed = FALSE)
wp_bologna_26_bib <- ReadBib("wp_bologna_26_references.bib", check = FALSE)

```

## {.center}

:::: {.columns}

::: {.column width="10%" .center}

&nbsp;

&nbsp;

&nbsp;

&nbsp;

[outline]{style="color:indianred"}

:::

::: {.column  width="90%" }

:::{.callout-tip appearance="minimal" icon=false title=""}


&nbsp;

### independence-based (IB) vs association-based (AB) dissimilarities

&nbsp;

### association-based for mixed data

&nbsp;

### taking into account continuous/categorical interactions

&nbsp;

### AB spectral clustering

&nbsp;

### example and future work

&nbsp;

### (adveRtising)

:::

:::

::::


##  

::: {.vcenter-slide}

<h3>[independence-based (IB)]{style="color:dodgerblue"} vs [association-based (AB)]{style="color:indianred"}  dissimilarities</h3>

:::
## 

[**learning from dissimilarities**]{style="color:indianred"}

some unsupervised learning methods take as input a dissimilarity matrix

&nbsp;

### [dimension reduction]{style="color:dodgerblue"}: multidimensional scaling (MDS)^[`r Citet(bib=wp_bologna_26_bib,"borg2005modern")`]

&nbsp;

### [clustering methods]{style="color:forestgreen"}: hierarchical (HC) and partitioning around medoids (PAM)^[`r Citet(bib=wp_bologna_26_bib,"kaufman2009finding")`]

&nbsp;

:::{.fragment .centered}
### the [dissimilarity]{style="color:indianred"} measure of choice is [key]{style="color:indianred"}, obviously 
:::


## 

[**intuition**]{style="color:indianred"}

2 continuous variables: add up by-variable  (absolute value or squared) differences 

```{r}
toy_data <- tibble(
  X1 = c(5, 3, 7),     
  X2 = c(1, 4, 2),     
  X3   = factor(c("purple", "blue", "gold")), 
  X4   = factor(c("A", "B", "C")) 
)  
```


```{r}
toy_data |> 
  ggplot(aes(x=X1,y=X2,label=X4)) +
  geom_point(size=8) + geom_text(color="white") +
  geom_point(inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=4,alpha=.001)+
  geom_point(inherit.aes = FALSE,aes(x=0,y=X2),color="forestgreen",size=4,alpha=.001)+theme_void()

```




## 

[**intuition**]{style="color:indianred"}

2 continuous variables: add up by-variable  (absolute value or squared) differences

```{r}
toy_data |> 
  ggplot(aes(x=X1,y=X2,label=X4)) +
  geom_point(size=8) + geom_text(color="white") +
  geom_point(inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=4)+
  geom_point(inherit.aes = FALSE,aes(x=0,y=X2),color="forestgreen",size=4)+theme_void()
```


## 

[**intuition**]{style="color:indianred"}

2 continuous variables: add up by-variable  (absolute value or squared) differences 

```{r}
toy_data |> 
  ggplot(aes(x=X1,y=X2,label=X4)) +
  geom_point(size=8) + geom_text(color="white") +
  # geom_point(inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=4)+
  # geom_point(inherit.aes = FALSE,aes(x=0,y=X2),color="forestgreen",size=4)+
  geom_line(data=toy_data |> filter(X4 %in%c("A","B")), 
               inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=2)+
  geom_line(data=toy_data |> filter(X4 %in%c("A","B")), 
               inherit.aes = FALSE,aes(y=X2,x=0),color="forestgreen",size=2)+
  annotate("label",x=2, y=2,label="d(A,B)",size=10)+theme_void()
  
```


## 

[**intuition**]{style="color:indianred"}

2 continuous variables: add up by-variable  (absolute value or squared) differences 

```{r}
toy_data |> 
  ggplot(aes(x=X1,y=X2,label=X4)) +
  geom_point(size=8) + geom_text(color="white") +
  # geom_point(inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=4)+
  # geom_point(inherit.aes = FALSE,aes(x=0,y=X2),color="forestgreen",size=4)+
  geom_line(data=toy_data |> filter(X4 %in%c("A","C")), 
               inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=2)+
  geom_line(data=toy_data |> filter(X4 %in%c("A","C")), 
               inherit.aes = FALSE,aes(y=X2,x=0),color="forestgreen",size=2)+
  annotate("label",x=2, y=2,label="d(A,C)",size=10)+theme_void()
  
```




## 

[**intuition**]{style="color:indianred"}

2 continuous variables: add up by-variable  (absolute value or squared) differences 

```{r}
toy_data |> 
  ggplot(aes(x=X1,y=X2,label=X4)) +
  geom_point(size=8) + geom_text(color="white") +
  # geom_point(inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=4)+
  # geom_point(inherit.aes = FALSE,aes(x=0,y=X2),color="forestgreen",size=4)+
  geom_line(data=toy_data |> filter(X4 %in%c("B","C")), 
               inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=2)+
  geom_line(data=toy_data |> filter(X4 %in%c("B","C")), 
               inherit.aes = FALSE,aes(y=X2,x=0),color="forestgreen",size=2)+
  annotate("label",x=2, y=2,label="d(B,C)",size=10)+theme_void()
  
```

<!-- ##  -->

<!-- [**intuition**]{style="color:indianred"} -->

<!-- 2 continuous and 1 categorical variables  -->

<!-- ```{r} -->
<!-- toy_data |>  -->
<!--   ggplot(aes(x=X1,y=X2,label=X4,color=as.character(X3))) + -->
<!--   geom_point(size=8) + geom_text(color="white") + -->
<!--   geom_point(inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=4,alpha=.001)+ -->
<!--   geom_point(inherit.aes = FALSE,aes(x=0,y=X2),color="forestgreen",size=4,alpha=.001)+ -->
<!--   scale_color_identity()+theme_void()+ -->
<!--   theme(legend.position="none") -->


<!-- ``` -->


## 

[**intuition**]{style="color:indianred"}

2 continuous and 1 categorical variables 

```{r}
toy_data |> 
  ggplot(aes(x=X1,y=X2,label=X4,color=as.character(X3))) +
  geom_point(size=8) + geom_text(color="white") +
  geom_point(inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=4,alpha=.001)+
  geom_point(inherit.aes = FALSE,aes(x=0,y=X2),color="forestgreen",size=4,alpha=.001)+
  scale_color_identity()+theme_void()+
  theme(legend.position="none")


```

## 

[**intuition**]{style="color:indianred"}

one might consider [purple]{style="color:purple"} and [blue]{style="color:blue"}  *closer* than e.g. [purple]{style="color:purple"} and [yellow]{style="color:gold"}

```{r}
toy_data |> 
  ggplot(aes(x=X1,y=X2,label=X4,color=as.character(X3))) +
  geom_point(size=8) + geom_text(color="white") +
  geom_point(inherit.aes = FALSE,aes(x=X1,y=0),color="dodgerblue",size=4,alpha=.001)+
  geom_point(inherit.aes = FALSE,aes(x=0,y=X2),color="forestgreen",size=4,alpha=.001)+
  scale_color_identity()+theme_void()+
  theme(legend.position="none")


```





## 

[**independence-based**]{style="color: dodgerblue"} 

Most commonly used dissimilarity (or, distance) measures are based on by-variable differences that are  then added together 

&nbsp;

-   ### in the continuous case: Euclidean or Manhattan distances

&nbsp;

-   ### in the categorical case: Hamming (matching) distance (among MANY others) 

&nbsp;

-   ### in the mixed data case:  Gower dissimilarity index

&nbsp;

:::{.fragment .centered}
### no inter-variable relations are considered $\rightarrow$ [independence-based]{style="color: dodgerblue"}
:::




## 

[**independence-based**]{style="color: dodgerblue"} 

-  When variables are correlated or associated, shared information is
effectively counted multiple times 

- inflated dissimilarities may cause potential distortions in downstream unsupervised learning tasks.


```{r}
library(tidyverse)
library(ggrepel)

cars_sel <- c("Toyota Corolla", "Cadillac Fleetwood")

mtcars_tbl <- mtcars |>
  rownames_to_column("car") |>
  mutate(
    disp_z = as.numeric(scale(disp)),
    wt_z   = as.numeric(scale(wt))
  )

# standardized matrix (consistent baseline)
Z <- mtcars_tbl |> select(disp_z, wt_z) |> as.matrix()

# Euclidean distance between selected cars (on standardized data)
seg_eucl <- mtcars_tbl |>
  filter(car %in% cars_sel) |>
  summarise(
    x    = disp_z[car == cars_sel[1]],
    y    = wt_z[car == cars_sel[1]],
    xend = disp_z[car == cars_sel[2]],
    yend = wt_z[car == cars_sel[2]],
    dist = sqrt((xend - x)^2 + (yend - y)^2)
  )

# Mahalanobis whitening (orientation-preserving) on standardized data
S <- cov(Z)
eig <- eigen(S)
U <- eig$vectors
D <- diag(eig$values)

W_pca <- Z %*% U %*% diag(1 / sqrt(diag(D))) %*% t(U)

whitened_tbl <- mtcars_tbl |>
  mutate(
    w1 = W_pca[, 1],
    w2 = W_pca[, 2]
  )

# segment in whitened space (Euclidean here == Mahalanobis in original standardized space)
seg_mah <- whitened_tbl |>
  filter(car %in% cars_sel) |>
  summarise(
    x    = w1[car == cars_sel[1]],
    y    = w2[car == cars_sel[1]],
    xend = w1[car == cars_sel[2]],
    yend = w2[car == cars_sel[2]],
    dist = sqrt((xend - x)^2 + (yend - y)^2)
  )
```

```{r}
ggplot(mtcars_tbl, aes(x = disp, y = wt)) +
  geom_point(size = 2) +
  labs(
    x = "Displacement (cu. in.)",
    y = "Weight (1000 lbs)",
    title = "Redundant continuous variables: displacement and weight",
    subtitle = "Larger cars tend to have both higher displacement and weight"
  ) +
  theme_minimal(base_size = 10)
```


## 

[**independence-based**]{style="color: dodgerblue"} 

-  When variables are correlated or associated, shared information is
effectively counted multiple times 

- inflated dissimilarities may cause potential distortions in downstream unsupervised learning tasks.


```{r}
ggplot(mtcars_tbl, aes(x = disp_z, y = wt_z)) +
  geom_point(size = 2) +
  stat_ellipse(type = "norm", level = 0.68, linewidth = .5,alpha=.25,color="indianred") +
  coord_equal() +
  labs(
    x = "Displacement (z-score)",
    y = "Weight (z-score)",
    title = "Same data after standardization",
    subtitle = "Units removed, redundancy remains"
  ) +
  theme_minimal(base_size = 10)
```

## 

[**independence-based**]{style="color: dodgerblue"} 

The [Euclidean distance]{style="color:dodgerblue"}   $\longrightarrow$  shared information is  over-counted

```{r}
ggplot(mtcars_tbl, aes(x = disp_z, y = wt_z)) +
  geom_point(size = 2, color = "grey70") +
  stat_ellipse(type = "norm", level = 0.68, linewidth = .5,alpha=.25,color="indianred") +
  geom_point(
    data = filter(mtcars_tbl, car %in% cars_sel),
    aes(color = car),
    size = 3
  ) +
  geom_segment(
    data = seg_eucl,
    aes(x = x, y = y, xend = xend, yend = yend),
    linewidth = 1,
    arrow = arrow(length = unit(0.15, "cm")),
    inherit.aes = FALSE
  ) +
  geom_text_repel(
    data = filter(mtcars_tbl, car %in% cars_sel),
    aes(label = car, color = car),
    size = 4,
    box.padding = 0.3,
    point.padding = 0.3,
    max.overlaps = Inf,
    show.legend = FALSE
  ) +
  geom_label(
    data = seg_eucl,
    aes(x = (x + xend)/2, y = (y + yend)/2, label = paste0("d_E = ", round(dist, 2))),
    inherit.aes = FALSE,
    label.size = 0,
    alpha = 0.85
  ) +
  coord_equal() +
  labs(
    x = "Displacement (z-score)",
    y = "Weight (z-score)",
    title = "Euclidean distance (standardized) overcounts redundant information",
    subtitle = "Differences along the shared 'size' direction are counted twice"
  ) +
  theme_minimal(base_size = 10) +
  theme(legend.position = "none")
```


## 

[**association-based**]{style="color: indianred"} 

The [Mahalanobis distance]{style="color:indianred"}  $\longrightarrow$ shared information is not over-counted


```{r}
ggplot(whitened_tbl, aes(w1, w2)) +
  geom_point(size = 2, color = "grey70") +
  stat_ellipse(type = "norm", level = 0.68, linewidth = .5,alpha=.25,color="dodgerblue") +
  geom_point(
    data = filter(whitened_tbl, car %in% cars_sel),
    aes(color = car),
    size = 4
  ) +
  geom_segment(
    data = seg_mah,
    aes(x = x, y = y, xend = xend, yend = yend),
    linewidth = 1,
    arrow = arrow(length = unit(0.15, "cm")),
    inherit.aes = FALSE
  ) +
  geom_text_repel(
    data = filter(whitened_tbl, car %in% cars_sel),
    aes(label = car, color = car),
    size = 4,
    box.padding = 0.35,
    point.padding = 0.3,
    max.overlaps = Inf,
    show.legend = FALSE
  ) +
  geom_label(
    data = seg_mah,
    aes(x = (x + xend)/2, y = (y + yend)/2, label = paste0("d_M = ", round(dist, 2))),
    inherit.aes = FALSE,
    label.size = 0,
    alpha = 0.85
  ) +
  labs(
    title = "Mahalanobis whitening",
    subtitle = "with preserved orientation (computed on standardized variables)",
    x = "whitened dim 1",
    y = "whitened dim 2"
  ) +
  coord_equal() +
  theme_minimal(base_size = 10) +
  theme(legend.position = "none")
```

:::{.fragment .centered}
### this is an [association-based]{style="color:indianred"} distance for continuous data
:::

## 

[**association-based**]{style="color: indianred"} pairwise distance

-   differences in line with the inter-variables association/correlation are down-weighted

:::{.callout-note  title="Association-based for continuous: Mahalanobis distance" icon=false}
 
Let ${\bf X}_{con}$  be $n\times Q_{d}$ a data matrix of $n$ observations described by $Q_{d}$ continuous variables, and let $\bf S$ the sample covariance matrix, the Mahalanobis distance matrix is

$$
{\bf D}_{mah}
= \left[\operatorname{diag}({\bf G})\,{\bf 1}_{n}^{\sf T}
+ {\bf 1}_{n}\,\operatorname{diag}({\bf G})^{\sf T}
- 2{\bf G}\right]^{\odot 1/2}
$$
 where 
 
 - $[\cdot]^{\odot 1/2}$ denotes the element-wise square root  
 
 - ${\bf G}=({\bf C}{\bf X}_{con}){\bf S}^{-1}({\bf C}{\bf X}_{con})^{\sf T}$ is the Mahalanobis Gram matrix 
 
 - ${\bf C}={\bf I}_{n}-\tfrac{1}{n}{\bf 1}_{n}{\bf 1}_{n}^{\sf T}$ is the centering operator 
 
:::


## 

[**association-based**]{style="color: indianred"} pairwise distance

-   differences in line with the inter-variables association/correlation are down-weighted

:::{.callout-tip title="Association-based for categorical: <span style='color: indianred;'>total variation distance (TVD)</span>^[`r Citet(bib=wp_bologna_26_bib,'le2005association')`]" icon=false}
To  distance matrix ${\bf D}_{tvd}$ is defined using the so-called <span style='color: dodgerblue;'>delta framework</span>^[`r Citet(bib=wp_bologna_26_bib,'vdv_pr')`] a general way to define categorical data distances.

Let ${\bf X}_{cat}$ be $n\times Q_{c}$ a data matrix of $n$ observations described by $Q_{c}$ categorical variables.

 
$$
{\bf D} = {\bf Z}{\Delta}{\bf Z}^{\sf T} 
= \left[\begin{array}{ccc} {\bf Z}_{1} & \dots & {\bf Z}_{Q_{c}} \end{array} \right]\left[\begin{array}{ccc}
                                                                                          {\bf\Delta}_1  & & \\
                                                                                          & \ddots &\\
                                                                                          & & {\bf\Delta}_{Q_{c}} \end{array} \right] \left[ \begin{array}{c}
                                                                                                                                             {\bf Z}_{1}^{\sf T}\\
                                                                                                                                             \vdots \\
                                                                                                                                             {\bf Z}_{Q_{c}}^{\sf T}
                                                                                                                                             \end{array} \right]
$$
- where ${\bf Z}=[{\bf Z}_1,\ldots,{\bf Z}_{Q_c}]$ is the super-indicator matrix, with $Q^{*}=\sum_{j=1}^{Q_c} q_j$

- ${\Delta}_j$ is the category dissimilarity matrix for variable $j$, i.e., the $j$th diagonal block of the block-diagonal matrix ${\Delta}$.

- setting ${\Delta}_j$ determines the categorical distance measure of choice (independent- or association-based)
:::


## 

[**association-based**]{style="color: indianred"} pairwise distance

-   differences in line with the inter-variables association/correlation are down-weighted



:::{.callout-tip title="Association-based for categorical: <span style='color: indianred;'>total variation distance (TVD)</span>^[`r Citet(bib=wp_bologna_26_bib,'le2005association')`] (2)" icon=false}

<!-- - In the [IB]{style="color: dodgerblue"} case, the off-diagonal elements of ${\Delta}_j$ depend only on variable $j$.  -->

<!-- - In the [AB]{style="color: indianred"} case, they depend on the association of each category pair with all other categorical variables.  -->

Consider the empirical joint probability distributions stored in the off-diagonal blocks of  ${\bf P}$:

$$
{\bf P} = \frac{1}{n} 
\begin{bmatrix}
{\bf Z}_1^{\sf T}{\bf Z}_1 & {\bf Z}_1^{\sf T}{\bf Z}_2 & \cdots & {\bf Z}_1^{\sf T}{\bf Z}_{Q_c} \\
\vdots & \ddots & \vdots & \vdots \\
{\bf Z}_{Q_c}^{\sf T}{\bf Z}_1 & {\bf Z}_{Q_c}^{\sf T}{\bf Z}_2 & \cdots & {\bf Z}_{Q_c}^{\sf T}{\bf Z}_{Q_c}
\end{bmatrix}.
$$

We refer to the conditional probability distributions for each variable $j$ given each variable $i$ ($i,j=1,\ldots,Q_c$, $i\neq j$),
stored in the block matrix

$$
{\bf R} = {\bf P}_z^{-1}({\bf P} - {\bf P}_z).
$$

where ${\bf P}_z = {\bf P} \odot {\bf I}_{Q^*}$, and ${\bf I}_{Q^*}$ is the $Q^*\times Q^*$ identity matrix.

:::


## 

[**association-based**]{style="color: indianred"} pairwise distance

-   differences in line with the inter-variables association/correlation are down-weighted

:::{.callout-tip title="Association-based for categorical: <span style='color: indianred;'>total variation distance (TVD)</span>^[`r Citet(bib=wp_bologna_26_bib,'le2005association')`] (3)" icon=false}

Let ${\bf r}^{ji}_a$ and ${\bf r}^{ji}_b$ be the rows of ${\bf R}_{ji}$, the $(j,i)$th off-diagonal block of ${\bf R}$
The category dissimilarity between $a$ and $b$ for variable $j$ based on the total variation distance (TVD) is defined as

$$
\delta^{j}_{tvd}(a,b)
= \sum_{i\neq j}^{Q_c} w_{ji}
\Phi^{ji}({\bf r}^{ji}_{a},{\bf r}^{ji}_{b})
= \sum_{i\neq j}^{Q_c} w_{ji}
\left[\frac{1}{2}\sum_{\ell=1}^{q_i}
|{\bf r}^{ji}_{a\ell}-{\bf r}^{ji}_{b\ell}|\right],
\label{ab_delta}
$$

where $w_{ji}=1/(Q_c-1)$ for equal weighting  (can be user-defined). 

 TVD-based dissimilarity matrix is, therefore, 

$$
{\bf D}_{tvd}= {\bf Z}{\Delta}^{(tvd)}{\bf Z}^{\sf T}.
$$

:::



##  

::: {.vcenter-slide}

<h2>association-based for mixed?</h2>

:::

## 

[**association-based**]{style="color: indianred"} for mixed

A straightforward AB-distance for mixed data is given by the convex combination of Mahalanobis and TVD distances:

$$
{\bf D}_{mix}
=\frac{Q_{d}}{Q}\,{\bf D}_{mah}
+\left(1-\frac{Q_{d}}{Q}\right){\bf D}_{tvd}.
$$

- this distance only accounts for correlations or associations among variables of the same type

- no continuous--categorical interactions are considered.

&nbsp;

:::{.fragment .centered}

### [how to measure interactions?]{style="color:dodgerblue"}

:::

## 

[**how to measure interactions**]{style="color:indianred"}

define  $\Delta^{int}$, that accounts for the interactions and augment $\Delta^{tvd}$

- the dissimilarity measure becomes 

$$
{\bf D}_{mix}^{(int)}
=
{\bf D}_{mah}
+
{\bf D}_{cat}^{(int)}.
$$

where 

$$
{\bf D}_{cat}^{(int)}={\bf Z}\tilde{\Delta}{\bf Z}^\top 
$$
and

$$
\tilde{\Delta} = (1-\alpha)\Delta^{tvd} + \alpha \Delta^{int}
$$
where $\alpha=\frac{1}{Q_{c}}$.




## 

[**how to measure interactions**]{style="color:indianred"}

What is  $\Delta^{int}$?


- the general entry for the $j^{th}$ diagonal block is $\delta_{int}^{j}(a,b)$ accounts for the interaction by measuring how the continuous variables help in discriminating between the observations choosing category $a$ and those choosing category $b$ for the $j^{th}$ categorical variable  

. . .

- consider the computation of  $\delta_{int}^{ij}\left({ab}\right)$  as a two-class ($a/b$) classification problem, with the continuous variables as predictors
  -  use a distance-based classifier: [nearest-neighbors]{style="color:dodgerblue"}


<!-- \frac{1}{2}\left(\right) --> 

<!-- ## -->

<!-- [**NN based computation of $\delta_{int}^{j}(a,b)$**]{style="color: indianred"} -->

<!-- ![](figures/phi_plots.png){fig-align="center"} -->


##  

[**$\Delta^{int}_{j}$**]{style="color:indianred"}

- consider ${\bf D}_{mah}$ and sort it  to identify the neighbors for each observation.

- set a proportion of neighbors to consider, say $\hat{\pi}_{nn}=0.1$
 

- for each pair of categories $(a,b)$, $a,b=1,\ldots,q_{j}$, $a\neq b$ of the $j^{th}$ categorical variable:


- classify the observations using the [prior corrected]{style="color:dodgerblue"}^[`r Citet(bib=wp_bologna_26_bib,"duda2001pattern")`] decision rule
    
    $$
     \text{if $i$ is such that }\ \ \ \frac{\hat{\pi}_{nn}(a)}{\hat{\pi}(a)}\geq\frac{\hat{\pi}_{nn}(b)}{\hat{\pi}(b)} \ \ \ \text{ then assign $i$ to class $a$ else to class $b$}
    $$
    
  
- compute the [balanced accuracy]{style="color:dodgerblue"}^[`r Citet(bib=wp_bologna_26_bib,"brodersen2010balanced")`] (average of class-wise sensitivities)
    $$
    \delta_{int}^{j}(a,b)=\frac{1}{2}\left(\frac{\texttt{true } a}{\texttt{true } a + \texttt{false }a}+\frac{\texttt{true } b}{\texttt{true } b + \texttt{false }b}\right)
    $$


##

[**well separated or not**]{style="color:indianred"}

![](figures/phi_plots.png){fig-align="center" width="70%"}


## 

[**Building $\Delta^{int}_{j}$**]{style="color:indianred"}


for the general categorical variable $j$ with $q_{j}$ you compute the quantities for $\frac{q_j(q_j -1)}{2}$ pairs


::: columns
::: {.column width="55%"}

<!-- ![](/figures/AB_plot.png){width=100%} -->
:::


::: {.column width="45%"}

&nbsp;

&nbsp;

&nbsp;

&nbsp;

$$
\Delta_{int} =
\begin{pmatrix}
0 & \cdot & \cdot & \cdot \\
\cdot & 0 & \cdot & \cdot \\
\cdot & \cdot & 0 &  \cdot\\
\cdot & \cdot & \cdot & 0
\end{pmatrix}
$$

:::
:::



## 

[**Building $\Delta^{int}_{j}$**]{style="color:indianred"}


for the general categorical variable $j$ with $q_{j}$ you compute the quantities for $\frac{q_j(q_j -1)}{2}$ pairs


::: columns
::: {.column width="55%"}

![](/figures/AB_plot.png){width=100%}
:::


::: {.column width="45%"}

&nbsp;

&nbsp;

&nbsp;

&nbsp;

$$
\Delta_{int} =
\begin{pmatrix}
0 & \color{indianred}{0.94} & \cdot & \cdot \\
\color{indianred}{0.94} & 0 & \cdot & \cdot \\
\cdot & \cdot & 0 &  \cdot\\
\cdot & \cdot & \cdot & 0
\end{pmatrix}
$$

:::
:::


## 

[**Building $\Delta^{int}_{j}$**]{style="color:indianred"}

for the general categorical variable $j$ with $q_{j}$ you compute the quantities for $\frac{q_j(q_j -1)}{2}$ pairs

::: columns
::: {.column width="55%"}

![](/figures/AC_plot.png){width=100%}
:::


::: {.column width="45%"}

&nbsp;

&nbsp;

&nbsp;

&nbsp;

$$
\Delta_{int} =
\begin{pmatrix}
0 & 0.94 & \color{indianred}{0.4} & \cdot \\
0.94 & 0 & \cdot & \cdot \\
\color{indianred}{0.4} & \cdot & 0 &  \cdot\\
\cdot & \cdot & \cdot & 0
\end{pmatrix}
$$

:::
:::


## 

[**Building $\Delta^{int}_{j}$**]{style="color:indianred"}

for the general categorical variable $j$ with $q_{j}$ you compute the quantities for $\frac{q_j(q_j -1)}{2}$ pairs

::: columns
::: {.column width="55%"}

![](/figures/AD_plot.png){width=100%}
:::


::: {.column width="45%"}

&nbsp;

&nbsp;

&nbsp;

&nbsp;

$$
\Delta_{int} =
\begin{pmatrix}
0 & 0.94 & 0.4 & \color{indianred}{0.39} \\
0.94 & 0 & \cdot & \cdot \\
0.4 & \cdot & 0 &  \cdot\\
\color{indianred}{0.39} & \cdot & \cdot & 0
\end{pmatrix}
$$

:::
:::


## 

[**Building $\Delta^{int}_{j}$**]{style="color:indianred"}

for the general categorical variable $j$ with $q_{j}$ you compute the quantities for $\frac{q_j(q_j -1)}{2}$ pairs

::: columns
::: {.column width="55%"}

![](/figures/BC_plot.png){width=100%}
:::


::: {.column width="45%"}

&nbsp;

&nbsp;

&nbsp;

&nbsp;

$$
\Delta_{int} =
\begin{pmatrix}
0 & 0.94 & 0.4 & 0.39 \\
0.94 & 0 & \color{indianred}{0.54} & \cdot \\
0.4 & \color{indianred}{0.54} & 0 & \cdot  \\
0.39 & \cdot & \cdot  & 0
\end{pmatrix}
$$

:::
:::


## 

[**Building $\Delta^{int}_{j}$**]{style="color:indianred"}

for the general categorical variable $j$ with $q_{j}$ you compute the quantities for $\frac{q_j(q_j -1)}{2}$ pairs

::: columns
::: {.column width="55%"}

![](/figures/BD_plot.png){width=100%}
:::


::: {.column width="45%"}

&nbsp;

&nbsp;

&nbsp;

&nbsp;

$$
\Delta_{int} =
\begin{pmatrix}
0 & 0.94 & 0.4 & 0.39 \\
0.94 & 0 & 0.54 & \color{indianred}{0.55} \\
0.4 & 0.54 & 0 & \cdot  \\
0.39 & \color{indianred}{0.55} & \cdot  & 0
\end{pmatrix}
$$

:::
:::


## 

[**Building $\Delta^{int}_{j}$**]{style="color:indianred"}

for the general categorical variable $j$ with $q_{j}$ you compute the quantities for $\frac{q_j(q_j -1)}{2}$ pairs

::: columns
::: {.column width="55%"}

![](/figures/CD_plot.png){width=100%}
:::


::: {.column width="45%"}

&nbsp;

&nbsp;

&nbsp;

&nbsp;

$$
\Delta_{int} =
\begin{pmatrix}
0 & 0.94 & 0.4 & 0.39 \\
0.94 & 0 & 0.54 & 0.55 \\
0.4 & 0.54 & 0 & \color{indianred}{0}  \\
0.39 & 0.55 & \color{indianred}{0}  & 0
\end{pmatrix}
$$

:::
:::

# Just one-way interaction?

## 

[**Just one-way interaction?**]{style="color:indianred"}

Let ${\bf X}=\left[{\bf X}_{con}{\bf X}_{cat} \right]$ be a mixed data matrix with $n$ observations described by $Q_{d}$ continuous and $Q_{c}$ categorical variables, respectively, and let  ${\bf x}_{i}=\left[{\bf x}_{i_{con}}{\bf x}_{i_{cat}}\right]$ the $i^{th}$ observation  

We build upon the following results

:::{.fragment}

::: {.callout-note  appearence="simple" icon=false}
## result A

The distribution of ${\bf x}_{i}$ can be written as
 
$$
f({\bf x}_{i_{con}},{\bf x}_{i_{cat}})=f({\bf x}_{i_{con}})f({\bf x}_{i_{cat}}\mid{\bf x}_{i_{con}})
$$
:::

:::


:::{.fragment}

::: {.callout-tip  appearence="simple" icon=false}
## result B

According to Hennig *et al.* (2019)^[`r Citet(bib=wp_bologna_26_bib,'Hennig19')`], starting from an arbitrary distance $d(x, c_{k})$   from a prototype, it  possible to construct a probabilistic clustering model^[`r Citet(bib=wp_bologna_26_bib,'ben2008probabilistic')`] as $f(x;c_{k},s_{k})=g(c_{k},s_{k})exp(-s_{k}d(x,c_{k})$, where $s_{k}$ is a concentration parameter.

Then if $f(\cdot)\sim N({\bf c}_{k}, {\bf S}_{k})$, and $d(\cdot,\cdot)$ the Mahalanobis distance, it results that 

$$
f({\bf x}_{i};{\bf c}_k, {\bf S}_k)=g({\bf c}_k, {\bf S}_k)\exp\left[-d({\bf x}_{i},{\bf c}_k,)\right]
$$
where $g({\bf c}_k, {\bf S}_k)$ is a normalization constant to make sure $f({\bf x}_{i};{\bf c}_k, {\bf S}_k)$ is a density function.


:::

:::

## 

[**Just one-way interaction?**]{style="color:indianred"}

::: {.callout-warning  appearence="simple" icon=false}
## result C

the dissimilarity between ${\bf x}_{i_{con}}$ and a generic cluster $k$
with center ${\bf c}_k$ and covariance matrix ${\bf S}_k$ is^[`r Citet(bib=wp_bologna_26_bib,'tortora2020probabilistic')`]

$$
d({\bf x}_{i_{con}},{\bf c}_k)=\log(M_{k} f({\bf x}_{i_{con}};{\bf c}_k, {\bf S}_k)^{-1})
$$ 

- $f(\cdot)$ a symmetric probability density function

- $M_k$ is the maximum of the density function

- that is, if ${\bf x}_{i_{con}}={\bf c}_k$ then $d({\bf x}_{i_{con}},{\bf c}_k)=0$.
:::


::: {.fragment}

replace ${\bf c}_{k}$ with a generic observation $i'$, the above becomes

$$
d({\bf x}_{i_{con}},{\bf x}_{i'_{con}})=\log(M f({\bf x}_{i_{con}};{\bf x}_{i'_{con}}, {\bf S})^{-1})
$$ 


where $M$ is the maximum of the density function.

:::

## 

[**Just one-way interaction...**]{style="color:indianred"}



Using the result C, it  can be shown that

$$
d({\bf x}_{i_{con}},{\bf x}_{i'_{con}})=\frac{1}{2}({\bf x}_{i_{con}}-{\bf x}_{i'_{con}}) {\bf S}^{-1}({\bf x}_{i_{con}}-{\bf x}_{i'_{con}})^{\sf T}
$$




::: {.fragment}

::: {.callout-tip  appearence="simple" icon=false}
## categorical analogue

consider the $Q_{cat}$-dimensional categorical vector ${\bf x}_{i_{cat}}$, it results, for its $j^{th}$ element, that

$$
p(x_{ij_{cat}}=a;  x_{i'j_{cat}}=b)  =  \left[\delta^{j}(a,b)\right]^{-1}
$$

where $a$ and $b$ are two general categories of the $jth$ variable

For the whole vector it results
$$
p({\bf x}_{ij_{cat}}; {\bf x}_{i'j_{cat}})  = \prod_{j=1}^{Q_c}  p(x_{ij_{cat}}=a_{j}; x_{i'j_{cat}}=b_{j})=\prod_{j=1}^{Q_c}\left[\delta^{j}(a_{j},b_{j})\right]^{-1}
$$


:::

:::


## 

[**Just one-way interaction... (wrap-up)**]{style="color:indianred"}


using the previous results, it is possible to define the dissimilarity between two mixed-data observations 

${\bf x}_i$ and  ${\bf x}_{i'}$

$$
d({\bf x}_{i}, {\bf x}_{i'})=\frac{1}{2}({\bf x}_{i_{con}}-{\bf x}_{i'_{con}}) {\bf S}^{-1}({\bf x}_{i_{con}}-{\bf x}_{i'_{con}})^{\sf T}-\log(p({\bf x}_{i_{cat}}|{\bf x}_{i_{con}}; {\bf x}_{i'_{cat}},  {\bf x}_{i'_{con}} ))
$$

that takes into account correlations, associations and cross-type interactions and is a equivalent to the genral entry of the previously defined

$$
{\bf D}_{mix}^{(int)}
=
{\bf D}_{mah}
+
{\bf D}_{cat}^{(int)}.
$$


# spectral clustering in a nutshell

## {.center}

[**Spectral clustering: a graph partitioning problem**]{style="color:indianred"}


::: {.callout-note  appearence="simple" icon=false}
## Graph representation

a graph representation of the data matrix $\bf X$: the aim is then to cut it into K groups (clusters)

```{r, warning=FALSE,message=FALSE,fig.align="center",out.width="35%"}
set.seed(123)
net = rgraph(4, mode = "graph", tprob = .75)
net = network(net, directed = FALSE)
# vertex names
network.vertex.names(net) = letters[1:4]
ggnet2(net, size = 12,color="dodgerblue", label = TRUE, label.size = 8,label.color = "white",edge.label = c("w_ac","w_cb","w_bd","w_cd"),
       edge.label.color = "indianred",edge.label.size = 12)
```

:::

. . .


::: {.callout-tip  appearence="simple" icon=false}
## the affinity matrix <span style = "color:dodgerblue"> ${\bf A}$ </span>
the elements <span style = "color:dodgerblue"> $\bf w_{ij}$ </span> of  <span style = "color:dodgerblue"> $\bf A$ </span> are high (low) if $i$ and $j$ are in the same (different) groups
```{r}
tib_graph = tibble(.=letters[1:4],a=c("0","0","w_ca","0"),b=c("0","0","w_cb","w_db"),c = c("w_ac","w_cb","0","w_dc"), d = c("0","w_bd","w_cd","0"))
tib_graph |> kbl(align = 'c')  |> kable_styling(full_width = F,font_size = 16) |> kable_minimal() |>   
 column_spec(c(1:5),
    border_right = T,
    border_left = T) |> 
  column_spec(c(2:5), color = "indianred") 
```

:::






## {.center} 

[**Spectral clustering: making the graph easy to cut**]{style="color:indianred"}


An approximated solution to the graph partitioning problem:

-  <span style = "color:dodgerblue"> spectral decomposition </span> of the graph Laplacian matrix, that is a [normalized]{style="color:dodgerblue"} version of the [affinity matrix ${\bf A}$]{style="color:dodgerblue"}:


$$\color{dodgerblue}{\bf{L}} = {\bf D}_{r}^{-1/2}\underset{\color{grey}{\text{affinity matrix } {\bf A}}}{\color{dodgerblue}{exp(-{\bf D}^{2}(2\sigma^{2})^{-1})}}{\bf D}_{r}^{-1/2} =\color{dodgerblue}{{\bf Q}{\Lambda}{\bf Q}^{\sf T}}$$



-  <span style = "color:dodgerblue"> $\bf D$ </span> be the $n\times n$ matrix of pairwise  distances
  
- the <span style = "color:dodgerblue"> $\sigma$ </span> parameter dictates the number of neighbors each observation is linked to (rule of thumb: median distance to the 20th nearest neighbor)
  
- diagonal terms of $\bf A$ are set to zero: <span style = "color:dodgerblue"> $a_{ii}=0$ </span>, $i=1,\ldots,n$ 
  
- <span style = "color:dodgerblue"> ${\bf D}_{r}=diag({\bf r})$ </span>,
  <span style = "color:dodgerblue"> ${\bf r}={\bf A}{\bf 1}$ </span> and
  <span style = "color:dodgerblue"> ${\bf 1}$ </span> is an $n$-dimensional vector of 1's 


. . .

- the spectral clustering of the $n$ original objects is a <span style = "color:dodgerblue"> $K$-means</span> applied on the rows of the matrix <span style = "color:dodgerblue"> ${\bf{\tilde Q}}$</span>, containing the first $K$ columns of $\bf Q$

<!-- ## {.center} -->

<!-- [**Spectral clustering: the NJW  procedure**]{style="color:indianred"} -->

<!-- - step 1: compute the pairwise distances matrix $\bf D$ -->

<!-- - step 2: switch to the affinity matrix $\bf A$ -->

<!-- - step 3: normalize the affinity matrix to obtain the Laplacian matrix $\bf L$ -->

<!-- - step 4: decompose $\bf L$ and obtain ${\bf {\tilde Q}}$, matrix of the $K$-dimensional spectral scores  -->

<!-- - step 5: apply the $K$-means on ${\bf {\tilde Q}}$ to obtain the cluster allocation vector -->


<!-- ## {.center} -->

<!-- [**scatterplot matrix of the spectral scores**]{style="color:indianred"} -->

<!-- ```{r,fig.align='center', echo=FALSE,out.width = "70%"} -->
<!-- p12 = multi_s_clusters %>% -->
<!--   ggplot(aes(x=spect_scores_1,y=spect_scores_2,color=spect_clust,shape=true_clust))+ -->
<!--   theme_minimal()+geom_point() + theme(legend.position = "none")+ xlab("")+ylab("")+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- p13 = multi_s_clusters %>% -->
<!--   ggplot(aes(x=spect_scores_1,y=spect_scores_3,color=spect_clust,shape=true_clust))+ -->
<!--   theme_minimal()+geom_point() + theme(legend.position = "none")+ xlab("")+ylab("")+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- p14 = multi_s_clusters %>% -->
<!--   ggplot(aes(x=spect_scores_1,y=spect_scores_4,color=spect_clust,shape=true_clust))+ -->
<!--   theme_minimal()+geom_point() + theme(legend.position = "none")+ xlab("")+ylab("")+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- p15 = multi_s_clusters %>% -->
<!--   ggplot(aes(x=spect_scores_1,y=spect_scores_5,color=spect_clust,shape=true_clust))+ -->
<!--   theme_minimal()+geom_point() + theme(legend.position = "none")+ xlab("")+ylab("")+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- p23 = multi_s_clusters %>% -->
<!--   ggplot(aes(x=spect_scores_2,y=spect_scores_3,color=spect_clust,shape=true_clust))+ -->
<!--   theme_minimal()+geom_point() + theme(legend.position = "none")+ xlab("")+ylab("")+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- p24 = multi_s_clusters %>% -->
<!--   ggplot(aes(x=spect_scores_2,y=spect_scores_4,color=spect_clust,shape=true_clust))+ -->
<!--   theme_minimal()+geom_point() + theme(legend.position = "none")+ xlab("")+ylab("")+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- p25 = multi_s_clusters %>% -->
<!--   ggplot(aes(x=spect_scores_2,y=spect_scores_5,color=spect_clust,shape=true_clust))+ -->
<!--   theme_minimal()+geom_point() + theme(legend.position = "none")+ xlab("")+ylab("")+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- p34 = multi_s_clusters %>% -->
<!--   ggplot(aes(x=spect_scores_3,y=spect_scores_4,color=spect_clust,shape=true_clust))+ -->
<!--   theme_minimal()+geom_point() + theme(legend.position = "none")+ xlab("")+ylab("")+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- p35 = multi_s_clusters %>% -->
<!--   ggplot(aes(x=spect_scores_3,y=spect_scores_5,color=spect_clust,shape=true_clust))+ -->
<!--   theme_minimal()+geom_point() + theme(legend.position = "none")+ xlab("")+ylab("")+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- p45 = multi_s_clusters %>% -->
<!--   ggplot(aes(x=spect_scores_4,y=spect_scores_5,color=spect_clust,shape=true_clust))+ -->
<!--   theme_minimal()+geom_point() + theme(legend.position = "none")+ xlab("")+ylab("")+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- blank_p = ggplot(data=tibble(x=0,y=0),mapping=aes(x,y))+ -->
<!--   theme_void()+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- blank_p1 = ggplot(data=tibble(x=0,y=0),mapping=aes(x,y))+ -->
<!--   theme_void()+geom_text(label="spectral score 1",size=4)+ -->
<!--   xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- blank_p2 = ggplot(data=tibble(x=0,y=0),mapping=aes(x,y))+ -->
<!--   theme_void()+geom_text(label="spectral score 2",size=4)+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- blank_p3 = ggplot(data=tibble(x=0,y=0),mapping=aes(x,y))+ -->
<!--   theme_void()+geom_text(label="spectral score 3",size=4)+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- blank_p4 = ggplot(data=tibble(x=0,y=0),mapping=aes(x,y))+ -->
<!--   theme_void()+geom_text(label="spectral score 4",size=4)+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- blank_p5 = ggplot(data=tibble(x=0,y=0),mapping=aes(x,y))+ -->
<!--   theme_void()+geom_text(label="spectral score 5",size=4)+xlim(c(-1,1))+ylim(c(-1,1)) -->

<!-- (blank_p1 | blank_p | blank_p |  blank_p | blank_p) /  -->
<!-- (p12 | blank_p2 | blank_p | blank_p | blank_p) / -->
<!-- (p13 | p23 | blank_p3 |  blank_p | blank_p) / -->
<!-- (p14 | p24 | p34 |  blank_p4 | blank_p) / -->
<!-- (p15 | p25 | p35 |  p45 | blank_p5)  -->



<!-- ``` -->

##

[**Spectral clustering: solution and performance**]{style="color:indianred"}

SC works well, particularly in case of non-convex and overlapping clusters^[`r Citet(bib=wp_bologna_26_bib,"murugesan2021benchmarking", before="e.g., ")`] 

::: {layout-ncol=2}
![](figures/parcoord_SC_solution.png){width=50%}

![](figures/SC_non_convex_solution.png){width=50%}

:::




```{r eval=FALSE}
files <- c(
  "data/ARIM_N1_1020.rdata",
  "data/ARIM_N2_1020.rdata",
  "data/ARIM_N1_2010.rdata",
  "data/ARIM_N2_2010.rdata",
  "data/ARIM_N1E.rdata",
  "data/ARIM_N2E.rdata",
  "data/ARIM_noise_N1.rdata",
  "data/ARIM_noise_N2.rdata",
  "data/ARIM_x_N1.rdata",
  "data/ARIM_x_N2.rdata"
)

files_old <- c(
  "data_old/ARIM_N1_1020.rdata",
  "data_old/ARIM_N2_1020.rdata",
  "data_old/ARIM_N1_2010.rdata",
  "data_old/ARIM_N2_2010.rdata",
  "data_old/ARIM_N1E.rdata",
  "data_old/ARIM_N2E.rdata",
  "data_old/ARIM_noise_N1.rdata",
  "data_old/ARIM_noise_N2.rdata",
  "data_old/ARIM_x_N1.rdata",
  "data_old/ARIM_x_N2.rdata"
)


ari_list <- setNames(
  lapply(files_old, function(f) {
    obj_name <- load(f)
    get(obj_name)
  }),
  tools::file_path_sans_ext(basename(files_old))
)


names(ari_list)= rep(c("more_cat","more_cont","balanced","balanced_noise","interaction"),each=2)

sc_scenario_params = tibble(scenario = c("more_cat","more_cont","balanced","balanced_noise","interaction"),
  cont_n = c(10,20,15,10,3),
       cat_n =  c(20,10,15,10,3),
       noise = c(0,0,0,10,0)
) |> crossing(obs_n = c(500,1000))


ari_tbl <- tibble(
  scenario = names(ari_list),
  ari = ari_list
) |>
  group_by(scenario) |>
  mutate(obs_n = c(500, 1000)[row_number()]) |>
  ungroup() |>
  mutate(
    ari_long = map(ari, \(x) {
      as_tibble(x) |>
        mutate(rep = row_number()) |>
        pivot_longer(
          cols = -rep,
          names_to = "measure",
          values_to = "ari"
        )
    })
  ) |>
  select(-ari) |>
  unnest(ari_long) |>
  left_join(sc_scenario_params, by = c("scenario", "obs_n"))


ari_tbl <- ari_tbl |>
  mutate(
    measure = dplyr::recode(
      measure,
      "Udep"     = "ab_dis",
      "Udep Int." = "ab_dis_int",
      "naive"    = "naive",
      "Gower"    = "gower",
      "mod G."   = "mod_gower"
    )
  )



library(dplyr)
library(tidyr)
library(purrr)
library(colorspace)
library(ggplot2)

methods <- sort(unique(ari_tbl$measure))
base_cols <- qualitative_hcl(length(methods))
names(base_cols) <- methods

method_cols <- c(
  setNames(lighten(base_cols, 0.3), paste0(methods, "_500")),
  setNames(darken(base_cols,  0.3), paste0(methods, "_1000"))
)

legend_breaks <- as.vector(rbind(
  paste0(methods, "_500"),
  paste0(methods, "_1000")
))

ari_plot <- ari_tbl |>
  mutate(
    scenario = factor(scenario,
                      levels = c("balanced","balanced_noise",
                                 "more_cat","more_cont",
                                 "interaction")),
    obs_n = factor(obs_n, levels = c(500, 1000)),
    method_n = paste0(measure, "_", obs_n),   # <-- MISSING LINE
    slide_group = case_when(
      scenario %in% c("balanced","balanced_noise") ~ "Balanced / Noise",
      scenario %in% c("more_cat","more_cont")     ~ "Unbalanced",
      scenario == "interaction"                   ~ "Interaction"
    )
  )

plot_ari_box <- function(dat) {
  ggplot(dat, aes(x = measure, y = ari)) +
    geom_boxplot(
      aes(fill = method_n, group = interaction(measure, obs_n)),
      position = position_dodge(width = 0.8),
      outlier.alpha = 0.4
    ) +
    facet_wrap(~ scenario, nrow = 1, scales = "free_x") +
    coord_cartesian(ylim = c(0, 1)) +
    scale_fill_manual(
      values = method_cols,
      breaks = legend_breaks,
      labels = gsub("_(500|1000)$", " (n=\\1)", legend_breaks)
    ) +
    labs(x = NULL, y = "ARI", fill = "Method") +
    theme_bw() +
    theme(legend.position = "none")
}

p_balanced <- ari_plot |> filter(slide_group == "Balanced / Noise") |> plot_ari_box()
p_unbal    <- ari_plot |> filter(slide_group == "Unbalanced")       |> plot_ari_box()
p_inter    <- ari_plot |> filter(slide_group == "Interaction")      |> plot_ari_box()

p_balanced
p_unbal
ggsave(file="figures/ab_interaction_plot.png",p_inter)
       
```
# a toy experiment

##

[**toy experiment**]{style="color:indianred"}: data generation

is $\delta_{int}^{j}(a,b)$  of help?


- three continuous signal, three continuous of Gaussian noise, three categorical variables (one noise)

- different dependence structures and location shifts in the signal continuous across the groups defined by the signal categorical variables.

- the continuous signal variables are generated conditionally on the signal categorical variables

$$
X_j = \beta_{0,hq} + \sum_{k>j}^{6} \beta_{1,hq} X_k, \qquad j = 1,2,3
$$
where $\beta_{1,hq}$ takes different values depending on the observed categories of the two signal categorical variables, $h$ and $q$,  respectively.

##


[**toy experiment**]{style="color:indianred"}

![](figures/scatterX.png){fig-align="center" width="60%"}

##

[**toy experiment**]{style="color:indianred"}: compared methods

-  [gower dissimilarity]{style="color: dodgerblue"}: a straightforward option

- [naive]{style="color: dodgerblue"} ^[`r Citet(bib=wp_bologna_26_bib,"mbuga21")`]
  
- [modified gower]{style="color: dodgerblue"}^[`r Citet(bib=wp_bologna_26_bib,"liu2024modified")`]

- [association-based approach]{style="color: dodgerblue"}^[`r Citet(bib=wp_bologna_26_bib,"vdv_jcgs")`]: with and without interaction 




##


[**toy experiment**]{style="color:indianred"}

![](figures/ab_interaction_plot.png){fig-align="center" width="60%"}



## {.center}

[**Final considerations and future work**]{style="color:indianred"}

- [association-based measures aim to go beyond match/mismatch of categories]{style="color:forestgreen"}

- [when the signal is limited to few variables, retrieving information from cont/cat interaction may be useful]{style="color:forestgreen"}

- [measuring interactions via non-parametric approach NN-based approach is suitable for non-convex/oddly shaped clusters]{style="color:forestgreen"}

- [computationally demanding (but it can be made bearable)]{style="color:dodgerblue"}

- [$\pi_{nn}$ tuning, regularization of $\delta_{int}$'s to reduce variability]{style="color:dodgerblue"}




<!-- ## {.center} -->


<!-- <h2>[an R package to compute distances: anydist?]{style="color:darkorange"}</h2> -->


## 

::: {.vcenter-slide}

<h2>
[an R package to compute distances: 
[m]{style="color:dodgerblue"}anydist!]{style="color:indianred"}
</h2>


&nbsp;

:::{.fragment}
<h3>
[manydist on CRAN (stable(-ish) version)](https://cran.r-project.org/web/packages/manydist/index.html)
</h3>

&nbsp;


<h3>
[manydist on GitHub version (for the latest updates)](https://github.com/alfonsoIodiceDE/manydist_package/tree/main/manydist)
</h3>

:::

:::

## general features

The `manydist` package is designed around a modular distance-based workflow.


| Component | Function / Interface | Role |
|----------|-------------|---------------|
| Core distance constructor | `mdist()` | Computes a pairwise dissimilarity matrix for mixed-type data: different indipendence-based and association based are specified, fully customizable |
| Recipe step | `step_mdist()` | Preprocessing step that computes an `mdist` dissimilarity matrix within a `recipe`, for seamless integration in learning pipelines. |
| k-nearest neighbors | `mdist_knn()` | Model specification for k-NN using a precomputed `mdist` dissimilarity matrix as input. Supports `tune` |
| leave-one-variable-out | `lovo_mdist()` | LOVO dissimilarity computations to assess by-variable contributions 


## forthcoming features

| Component | Function / Interface | Role |
|----------|-------------|---------------|
| Partitioning around medoids | `mdist_pam()` | Model specification for PAM clustering based on `mdist` dissimilarities. 
| spectral clustering | `mdist_spectral()` | Model specification for spectral clustering based on `mdist` dissimilarities. 
| and more to come... |  ||

## 

**main references**

```{r ,echo=FALSE, results='asis', size='tiny'}
PrintBibliography(wp_bologna_26_bib[c("Hennig19","le2005association","mbuga21","mousavi2023generalized","murugesan2021benchmarking","ng2001spectral","vdv_pr","vdv_jcgs")] )
```



## Acknowledgments


*The publication was produced with funding from the Italian Ministry of University and Research under the Call for Proposals related to the scrolling of the final rankings of the PRIN 2022 call.*

 **Latent variable models and dimensionality reduction methods for complex data** (PI Prof. Paolo Giordani) 

**Project No. 20224CRB9E, CUP C53C24000730006** 

**Grant Assignment Decree No. 1401 adopted on 18.9.2024 by MUR**

&nbsp;

&nbsp;

&nbsp;

&nbsp;



<h4>[you can check this presentation at]{style="color:indianred"}</h4>

<h4>[https://alfonsoiodicede.github.io/Workshop_Bologna_26_pres_SC_mix.html]{style="color:dodgerblue"}
</h4>




<!-- . . . -->

<!-- <p align="right">slides available at [https://alfonsoiodicede.github.io/](https://alfonsoiodicede.github.io/talks.html) -->
<!-- </p> -->










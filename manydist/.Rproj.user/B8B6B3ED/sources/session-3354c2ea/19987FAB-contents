---
title: "Educational Resilience Clustering Analysis using Akhanli & Hennig Framework"
subtitle: "Cycle-Specific Mixed-Type Distance Clustering for PISA Disadvantaged Students"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  html_document:  
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    theme: united
    highlight: tango
    code_folding: show
  pdf_document:
    toc: true
    number_sections: true
params:
  data_dir: "/Users/amarkos/PISA_Data/"
  target_countries: "GRC"
  disadvantaged_threshold: 0.25
  k_range_min: 2
  k_range_max: 25
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 12,
  fig.height = 8,
  cache = TRUE,
  comment = NA
)

# Set global options
options(scipen = 999)
```

# Executive Summary

This analysis adapts the Akhanli & Hennig (2020) composite clustering validity framework to identify educational resilience profiles among disadvantaged PISA students using cycle-specific clustering with multiple distance methods to account for variable differences across PISA waves.

# Library Setup and Configuration

```{r libraries, message=FALSE, warning=FALSE}
cat("=== INITIALIZING ENHANCED CLUSTERING ENVIRONMENT ===\n")

# ---- Core Libraries for Advanced Mixed-Type Clustering ----
library(haven)       # SPSS file reading
library(data.table)  # High-performance data operations
library(dtplyr)      # dplyr syntax with data.table backend
library(labelled)    # Handle SPSS labels
library(psych)       # Descriptive statistics
library(ggplot2)     # Visualizations
library(tidyr)       # Data reshaping
library(dplyr)       # Data manipulation
library(gridExtra)   # Plot arrangements
library(viridis)     # Color palettes
library(scales)      # Scaling functions
library(fpc)         # Clustering validation - KEY for Akhanli & Hennig framework
library(mclust)      # Adjusted Rand Index
library(fastDummies) # Dummy variable creation
library(mice)        # Multiple imputation
library(corrplot)    # Correlation visualization
library(factoextra)  # Enhanced clustering visualization
library(cluster)     # Additional clustering methods

# ---- Advanced Distance Calculation Package ----
if (!requireNamespace("manydist", quietly = TRUE)) {
  stop("manydist package required but not available. Install with: install.packages('manydist')")
}
library(manydist)

# ---- Clustering Methods ----
library(dbscan)      # Density-based clustering
library(kernlab)     # Spectral clustering

cat("✓ Enhanced clustering libraries loaded successfully\n")
cat("✓ manydist package loaded for advanced mixed-type distances\n")
cat("✓ fpc package loaded for Akhanli & Hennig validation framework\n")

# ---- Configuration ----
setDTthreads(0) # Use all available cores
set.seed(42)   # Reproducibility
cat("✓ Analysis environment configured\n")
```

# Analysis Parameters

```{r params, echo=FALSE}
cat("\n=== CONFIGURING ANALYSIS PARAMETERS ===\n")

data_dir <- params$data_dir
target_countries <- params$target_countries
DISADVANTAGED_THRESHOLD <- params$disadvantaged_threshold

# Path helper function
path <- function(fname) file.path(data_dir, fname)

# Enhanced clustering parameters for cycle-specific analysis
CYCLE_CLUSTERING_PARAMS <- list(
  # Macro-level clustering (policy-oriented)
  macro = list(
    #  k_min = 3,
    #  k_max = 6,
    k_min = 3,
    k_max = 6,
    description = "Broad resilience profiles for policy analysis"
  ),
  
  # Micro-level clustering (intervention-oriented) 
  micro = list(
    k_min = 8,
    k_max = 12,
    description = "Fine-grained profiles for targeted interventions"
  ),
  
  # Validation parameters (THIS DOESNT PLAY ANY ROLE)
  validation = list(
    n_bootstrap = 25,
    n_random_clusterings = 10,
    stability_method = "bootstrap"
  )
)

# Distance methods - Your original three approaches
DISTANCE_METHODS <- list(
  
  # Method 1: Unbiased Independent
  unbiased_independent = list(
    name = "Unbiased_Independent",
    params = list(
      preset = "custom",
      distance_cont = "manhattan",
      distance_cat = "matching", 
      commensurable = TRUE,
      scaling_cont = "std"
    ),
    description = "Commensurable Manhattan (numerical) + Simple matching (categorical)"
  ),
  
  # Method 2: Unbiased Dependent  
  unbiased_dependent = list(
    name = "Unbiased_Dependent",
    params = list(
      preset = "custom",
      distance_cont = "manhattan", 
      distance_cat = "tot_var_dist",
      commensurable = TRUE,
      scaling_cont = "pc_scores"
    ),
    description = "Commensurable PCA-scaled (numerical) + Total variance (categorical)"
  ),
  
  # Method 3: Gower baseline
  gower_baseline = list(
    name = "Gower_Baseline",
    params = list(
      preset = "gower",
      commensurable = FALSE
    ),
    description = "Standard Gower distance"
  )
)

cat("✓ Cycle-specific clustering parameters configured\n")
```

# Data Loading and Processing

```{r data_loading, message=FALSE, warning=FALSE}
cat("\n=== LOADING AND PROCESSING PISA DATA ===\n")

# ---- PISA Cycle Configuration ----
pisa_cycles <- list(
  "2015" = list(
    student_file = "CY6_MS_CMB_STU_QQQ.sav",
    school_file = "CY6_MS_CMB_SCH_QQQ.sav",
    year = 2015,
    var_mapping = list(
      student = c(
        "hisei" = "HISEI",
        "PARED" = "PAREDINT",
        "ANXTEST" = "ANXMAT",
        "MOTIVAT" = "MATHMOT"
      ),
      school = c(
        "STRATIO" = "STRATIO"
      )
    )
  # ),
  # "2018" = list(
  #   student_file = "CY07_MSU_STU_QQQ.sav",
  #   school_file = "CY07_MSU_SCH_QQQ.sav",
  #   year = 2018,
  #   var_mapping = list(
  #     student = c(),
  #     school = c(
  #       "STRATIO" = "STRATIO"
  #     )
  #   )
  # ),
  # "2022" = list(
  #   student_file = "CY08MSP_STU_QQQ.sav",
  #   school_file = "CY08MSP_SCH_QQQ.sav",
  #   year = 2022,
  #   var_mapping = list(
  #     student = c(),
  #     school = c(
  #       "SMRATIO" = "STRATIO"
  #     )
  #   )
   )
)

# ---- Core Variable Lists ----
core_student_vars <- list(
  ids = c("CNT", "CNTSCHID", "CNTSTUID"),
  achievement = c(
    paste0("PV", 1:10, "MATH"),
    paste0("PV", 1:10, "READ"),
    paste0("PV", 1:10, "SCIE")
  ),
  ses_background = c(
    "ESCS", "HISEI", "PAREDINT", "HOMEPOS", "ICTRES"
  ),
  demographics = c(
    "ST004D01T", "IMMIG"
  ),
  motivation = c(
    "ANXMAT", "MATHMOT", "MATHEFF", "SCIEEFF", "JOYSCIE", "INTMAT",
    "GRWTHMND", "WORKMAST", "RESILIENCE", "MASTGOAL", "GFOFAIL", "COMPETE"
  ),
  behavior = c(
    "PERSEV", "TRUANCY", "HOMWRK", "DISCLIMA", "DIRINS", "PERFEED", "OUTHOURS"
  ),
  social_emotional = c(
    "BELONG", "TEACHSUP", "PEERREL", "BULLIED", "FAMSUP", "RELATST", 
    "EMOSUPS", "PERCOMP", "PERCOOP", "TEACHINT"
  ),
  resilience_other = c(
    "LIFESAT", "EUDMO", "WELLBEING", "ATTSCHL", "EXPDEG", "PHYSACT", "SCHWELL"
  ),
  weights = "W_FSTUWT"
)

core_school_vars <- list(
  ids = c("CNT", "CNTSCHID"),
  characteristics = c(
    "SCHSIZE", "SCHLTYPE", "STRATIO"
  ),
  resources = c(
    "STAFFSHORT", "EDUSHORT", "RATCMP1"
  ),
  weights = "W_SCHGRNRABWT"
)

# ---- File Reading Function ----
safe_read_pisa_with_mapping <- function(file_path, var_list, var_mapping,
                                        target_countries = NULL, cycle_year = NULL) {
  tryCatch({
    file_vars <- names(read_sav(file_path, n_max = 0))
    reverse_mapping <- setNames(names(var_mapping), var_mapping)
    vars_to_request <- sapply(var_list, function(std_var) {
      if (std_var %in% names(reverse_mapping)) {
        reverse_mapping[[std_var]]
      } else {
        std_var
      }
    })
    available_vars <- intersect(vars_to_request, file_vars)
    if (length(available_vars) == 0) {
      warning("No requested variables found in ", basename(file_path))
      return(NULL)
    }
    dt <- setDT(read_sav(file_path, col_select = any_of(available_vars)))
    for (old_name in names(var_mapping)) {
      new_name <- var_mapping[[old_name]]
      if (old_name %in% names(dt)) {
        setnames(dt, old_name, new_name)
      }
    }
    if (!is.null(target_countries) && "CNT" %in% names(dt)) {
      dt <- dt[CNT %in% target_countries]
    }
    if (!is.null(cycle_year)) {
      dt[, CYCLE := cycle_year]
    }
    return(dt)
  }, error = function(e) {
    cat("Error reading file:", basename(file_path), "\n")
    return(NULL)
  })
}

# ---- Load All PISA Cycles ----
all_student_vars <- unlist(core_student_vars, use.names = FALSE)
all_school_vars <- unlist(core_school_vars, use.names = FALSE)
pisa_student_list <- list()
pisa_school_list <- list()

for (cycle_name in names(pisa_cycles)) {
  cycle_info <- pisa_cycles[[cycle_name]]
  cat("\n--- Loading PISA", cycle_info$year, "---\n")
  
  stu_file <- path(cycle_info$student_file)
  stu_data <- safe_read_pisa_with_mapping(
    stu_file, all_student_vars, cycle_info$var_mapping$student,
    target_countries, cycle_info$year
  )
  if (!is.null(stu_data) && nrow(stu_data) > 0) {
    pisa_student_list[[cycle_name]] <- stu_data
    cat("Students loaded:", nrow(stu_data), "\n")
  }
  
  sch_file <- path(cycle_info$school_file)
  sch_data <- safe_read_pisa_with_mapping(
    sch_file, all_school_vars, cycle_info$var_mapping$school,
    target_countries, cycle_info$year
  )
  if (!is.null(sch_data) && nrow(sch_data) > 0) {
    pisa_school_list[[cycle_name]] <- sch_data
    cat("Schools loaded:", nrow(sch_data), "\n")
  }
}

# ---- Process PISA Cycle Function ----
process_pisa_cycle <- function(stu_dt, sch_dt, cycle_year) {
  if (is.null(stu_dt) || nrow(stu_dt) == 0) return(NULL)
  
  # Compute average achievement scores
  stu_dt[, MATH_AVG := rowMeans(.SD, na.rm = TRUE), .SDcols = patterns("MATH$")]
  stu_dt[, READ_AVG := rowMeans(.SD, na.rm = TRUE), .SDcols = patterns("READ$")]
  stu_dt[, SCIE_AVG := rowMeans(.SD, na.rm = TRUE), .SDcols = patterns("SCIE$")]
  
  # Identify disadvantaged students
  escs_quartile <- quantile(stu_dt$ESCS, probs = DISADVANTAGED_THRESHOLD, na.rm = TRUE)
  stu_dt[, DISADVANTAGED := ifelse(ESCS <= escs_quartile, 1, 0)]
  
  # Define resilience
  MATH_LEVEL3_THRESHOLD <- 482.38
  READ_LEVEL3_THRESHOLD <- 480.18
  SCIE_LEVEL3_THRESHOLD <- 484.14
  
  stu_dt[DISADVANTAGED == 1, RESILIENT := ifelse(
    MATH_AVG >= MATH_LEVEL3_THRESHOLD & 
      READ_AVG >= READ_LEVEL3_THRESHOLD & 
      SCIE_AVG >= SCIE_LEVEL3_THRESHOLD, 1, 0)]
  
  # Filter to disadvantaged students
  disadv_dt <- stu_dt[DISADVANTAGED == 1]
  
  # Merge with school data
  if (!is.null(sch_dt) && nrow(sch_dt) > 0) {
    disadv_dt <- merge(disadv_dt, sch_dt, by = c("CNT", "CNTSCHID", "CYCLE"), all.x = TRUE)
  }
  
  disadv_dt[is.na(RESILIENT), RESILIENT := 0]
  
  return(disadv_dt)
}

# ---- Apply Processing ----
processed_list <- mapply(process_pisa_cycle, pisa_student_list, pisa_school_list, 
                         names(pisa_cycles), SIMPLIFY = FALSE)

cat("\n=== DATA PROCESSING SUMMARY ===\n")
for (cycle in names(processed_list)) {
  if (!is.null(processed_list[[cycle]])) {
    n_students <- nrow(processed_list[[cycle]])
    n_resilient <- sum(processed_list[[cycle]]$RESILIENT, na.rm = TRUE)
    resilience_rate <- round(n_resilient / n_students * 100, 1)
    
    cat("- Cycle", cycle, ":", n_students, "students,", n_resilient, 
        "resilient (", resilience_rate, "%)\n")
  }
}
```

# Cycle-Specific Variable Selection

```{r variable_selection, message=FALSE, warning=FALSE}
cat("\n=== ENHANCED CYCLE-SPECIFIC VARIABLE SELECTION ===\n")

# Function to comprehensively analyze and select variables for each cycle
identify_cycle_variables_enhanced <- function(cycle_data, cycle_name) {
  
  cat(sprintf("→ Comprehensive variable analysis for PISA %s...\n", cycle_name))
  
  # Get all available variables
  all_available_vars <- names(cycle_data)
  cat(sprintf("  Total variables in dataset: %d\n", length(all_available_vars)))
  
  # Core resilience variables (must have)
  core_vars <- c("MATH_AVG", "READ_AVG", "SCIE_AVG", "ESCS", "RESILIENT")
  
  # Comprehensive extended variable categories
  extended_vars <- list(
    
    # Socioeconomic background
    ses_background = c("HISEI", "PAREDINT", "HOMEPOS", "ICTRES", "CULTPOSS", "HEDRES"),
    
    # Demographics (basic student characteristics)
    demographics = c("ST004D01T", "IMMIG", "PRESCHOOL", 
                     "ST005Q01TA", "ST003D02T", "ST003D03T"),
    
    # Academic motivation and attitudes
    motivation = c("ANXMAT", "MATHMOT", "MATHEFF", "SCIEEFF", "JOYSCIE", "INTMAT",
                   "GRWTHMND", "WORKMAST", "RESILIENCE", "MASTGOAL", "GFOFAIL", "COMPETE",
                   "SWBP", "MOTIVAT", "CONFID"),
    
    # Learning behaviors and study habits
    learning_behavior = c("PERSEV", "TRUANCY", "HOMWRK", "DISCLIMA", "DIRINS", 
                          "PERFEED", "OUTHOURS", "STUDYEFF", "METASUM", "METACOG"),
    
    # Social-emotional factors
    social_emotional = c("BELONG", "TEACHSUP", "PEERREL", "BULLIED", "FAMSUP", "RELATST", 
                         "EMOSUPS", "PERCOMP", "PERCOOP", "TEACHINT", "STUREL", "SWBP"),
    
    # Well-being and life satisfaction
    wellbeing = c("LIFESAT", "EUDMO", "WELLBEING", "ATTSCHL", "EXPDEG", "PHYSACT", 
                  "SCHWELL", "JOYREAD", "WORRYSCH"),
    
    # School context and environment
    school_context = c("SCHSIZE", "SCHLTYPE", "STRATIO", "STAFFSHORT", "EDUSHORT", 
                       "RATCMP1", "PROPCERT", "PROPQUAL", "TCSHORT"),
    
    # Technology and ICT
    ict_usage = c("USESCH", "USEHOME", "ICTSCHOOL", "ICTOUTSIDE", "ICTCLASS")
  )
  
  # Pattern-based variable discovery
  learning_patterns <- c("LEARN", "STUDY", "COGN", "META", "THINK", "REASON")
  motivation_patterns <- c("MOTIV", "ANXIE", "CONF", "ENJOY", "LIKE", "INTER")
  social_patterns <- c("PEER", "FRIEND", "SOCIAL", "COOP", "COMP", "HELP")
  wellbeing_patterns <- c("WELL", "LIFE", "HAPPY", "STRESS", "WORRY", "FEEL")
  
  # Find variables matching patterns
  pattern_vars <- character(0)
  for (pattern in c(learning_patterns, motivation_patterns, social_patterns, wellbeing_patterns)) {
    pattern_matches <- all_available_vars[grepl(pattern, all_available_vars, ignore.case = TRUE)]
    pattern_vars <- c(pattern_vars, pattern_matches)
  }
  
  # Remove system variables and IDs from pattern matches
  exclude_patterns <- c("^PV[0-9]", "^W_", "^CNT", "ID$", "^ST00[0-9]Q[0-9][0-9]", "FLAG", "ADMIN")
  for (exclude_pattern in exclude_patterns) {
    pattern_vars <- pattern_vars[!grepl(exclude_pattern, pattern_vars)]
  }
  
  extended_vars$pattern_based <- unique(pattern_vars)
  
  # Check availability and coverage for each category
  available_vars <- all_available_vars
  cycle_clustering_vars <- core_vars[core_vars %in% available_vars]
  
  category_summary <- list()
  
  cat("\n  Category analysis:\n")
  
  for (category in names(extended_vars)) {
    category_vars <- extended_vars[[category]]
    available_category_vars <- intersect(category_vars, available_vars)
    
    category_summary[[category]] <- list(
      total = length(category_vars),
      available = length(available_category_vars),
      variables = available_category_vars,
      coverage = if(length(category_vars) > 0) length(available_category_vars) / length(category_vars) else 0
    )
    
    # Flexible inclusion criteria
    include_category <- FALSE
    
    if (category == "pattern_based") {
      include_category <- length(available_category_vars) > 0
    } else {
      include_category <- category_summary[[category]]$coverage >= 0.3
    }
    
    if (include_category) {
      cycle_clustering_vars <- c(cycle_clustering_vars, available_category_vars)
      status_symbol <- "✓"
    } else {
      status_symbol <- "✗"
    }
    
    cat(sprintf("    %s %s: %d/%d variables (%.1f%% coverage)\n", 
                status_symbol, category, 
                length(available_category_vars), length(category_vars),
                category_summary[[category]]$coverage * 100))
    
    # Show some example variables for each category
    if (length(available_category_vars) > 0) {
      example_vars <- head(available_category_vars, 3)
      cat(sprintf("      Examples: %s\n", paste(example_vars, collapse = ", ")))
    }
  }
  
  # Remove duplicates
  cycle_clustering_vars <- unique(cycle_clustering_vars)
  
  # Additional comprehensive variable discovery
  cat("\n  → Searching for additional educational variables...\n")
  
  # Look for any remaining educational/psychological variables
  educational_keywords <- c("TEACH", "CLASS", "INSTRU", "CURRIC", "ASSESS", "FEEDB",
                            "ATTITUD", "BELIE", "PERCE", "EXPEC", "GOAL", "STRAT")
  
  additional_vars <- character(0)
  for (keyword in educational_keywords) {
    keyword_matches <- all_available_vars[grepl(keyword, all_available_vars, ignore.case = TRUE)]
    # Filter out system variables
    keyword_matches <- keyword_matches[!grepl("^PV[0-9]|^W_|^CNT|ID$|FLAG|ADMIN", keyword_matches)]
    additional_vars <- c(additional_vars, keyword_matches)
  }
  
  # Add variables not already included
  new_additional_vars <- setdiff(additional_vars, cycle_clustering_vars)
  if (length(new_additional_vars) > 0) {
    cycle_clustering_vars <- c(cycle_clustering_vars, new_additional_vars)
    cat(sprintf("  → Found %d additional educational variables\n", length(new_additional_vars)))
    cat(sprintf("      Examples: %s\n", paste(head(new_additional_vars, 5), collapse = ", ")))
  }
  
  # Final deduplication
  cycle_clustering_vars <- unique(cycle_clustering_vars)
  
  # Remove any system/ID variables that might have slipped through
  final_exclude_pattern <- "^(PV[0-9]|W_|CNT|.*ID$|.*FLAG|.*ADMIN)"
  cycle_clustering_vars <- cycle_clustering_vars[!grepl(final_exclude_pattern, cycle_clustering_vars)]
  
  cat(sprintf("\n✓ Selected %d clustering variables for PISA %s\n", 
              length(cycle_clustering_vars), cycle_name))
  
  # Show variable breakdown by type
  suspected_categorical <- cycle_clustering_vars[grepl("^ST[0-9].*Q[0-9]|IMMIG|SCHLTYPE", cycle_clustering_vars)]
  suspected_continuous <- setdiff(cycle_clustering_vars, suspected_categorical)
  
  
  cat(sprintf("  → Estimated: %d continuous, %d categorical\n", 
              length(suspected_continuous), length(suspected_categorical)))
  
  return(list(
    clustering_vars = cycle_clustering_vars,
    category_summary = category_summary,
    suspected_categorical = suspected_categorical,
    suspected_continuous = suspected_continuous,
    total_available = length(all_available_vars)
  ))
}

# Enhanced analysis for each cycle
cycle_variable_analysis <- list()

for (cycle_name in names(processed_list)) {
  if (!is.null(processed_list[[cycle_name]])) {
    cycle_variable_analysis[[cycle_name]] <- identify_cycle_variables_enhanced(
      processed_list[[cycle_name]], cycle_name
    )
  }
}

cat("\n✓ Enhanced variable analysis completed for all cycles\n")

# Detailed comparison across cycles
cat("\n=== CROSS-CYCLE VARIABLE COMPARISON ===\n")

all_unique_vars <- unique(unlist(lapply(cycle_variable_analysis, function(x) x$clustering_vars)))
cat(sprintf("Total unique clustering variables across all cycles: %d\n\n", length(all_unique_vars)))

# Create variable availability matrix
cycle_names <- names(cycle_variable_analysis)
var_matrix <- matrix(FALSE, nrow = length(all_unique_vars), ncol = length(cycle_names))
rownames(var_matrix) <- all_unique_vars
colnames(var_matrix) <- cycle_names

for (i in seq_along(cycle_names)) {
  cycle_vars <- cycle_variable_analysis[[cycle_names[i]]]$clustering_vars
  var_matrix[cycle_vars, i] <- TRUE
}

# Summary statistics
cat("Variables available by cycle:\n")
for (cycle_name in cycle_names) {
  n_vars <- sum(var_matrix[, cycle_name])
  cat(sprintf("  %s: %d variables\n", cycle_name, n_vars))
}

# Variables common to all cycles
common_vars <- all_unique_vars[rowSums(var_matrix) == length(cycle_names)]
cat(sprintf("\nVariables common to all cycles: %d\n", length(common_vars)))
if (length(common_vars) <= 10) {
  cat("  Common variables:", paste(common_vars, collapse = ", "), "\n")
} else {
  cat("  Common variables (first 10):", paste(head(common_vars, 10), collapse = ", "), "\n")
}

cat("\n✓ Cross-cycle variable comparison completed\n")
```


# Cycle-Specific Data Preparation with MICE Imputation

```{r mice_imputation, message=FALSE, warning=FALSE}
cat("\n=== CYCLE-SPECIFIC DATA PREPARATION WITH MICE IMPUTATION ===\n")

# Enhanced data preparation function with MICE imputation - FIXED for haven_labelled
prepare_cycle_clustering_data <- function(cycle_data, cycle_vars, cycle_name, 
                                          missing_threshold = 0.3, mice_iterations = 5) {
  
  cat(sprintf("→ Preparing clustering dataset for PISA %s...\n", cycle_name))
  
  # Essential columns
  essential_cols <- c("RESILIENT", "CYCLE", "W_FSTUWT", "CNTSTUID", "CNTSCHID")
  available_essential <- intersect(essential_cols, names(cycle_data))
  
  # Combine variables
  all_vars <- c(cycle_vars, available_essential)
  clustering_data <- cycle_data[, all_vars, with = FALSE]
 # clustering_data <- clustering_data[1:450,]
  # IMPORTANT: Convert all haven_labelled columns to regular numeric/factor
  for (col_name in names(clustering_data)) {
    if (inherits(clustering_data[[col_name]], "haven_labelled")) {
      # Convert to numeric (you could also use as_factor() for categorical)
      clustering_data[[col_name]] <- as.numeric(clustering_data[[col_name]])
    }
  }
  
  # Step 1: Analyze missing value patterns
  cat("→ Analyzing missing value patterns...\n")
  
  missing_summary <- clustering_data[, lapply(.SD, function(x) sum(is.na(x))/length(x)), 
                                     .SDcols = cycle_vars]
  
  # Step 2: Filter variables based on missing threshold (30%)
  good_vars <- names(missing_summary)[missing_summary <= missing_threshold]
  dropped_vars <- names(missing_summary)[missing_summary > missing_threshold]
  
  if (length(dropped_vars) > 0) {
    cat(sprintf("⚠ Dropped %d variables with >%.0f%% missingness: %s\n", 
                length(dropped_vars), missing_threshold * 100,
                paste(head(dropped_vars, 3), collapse = ", ")))
    if (length(dropped_vars) > 3) {
      cat(sprintf("  ... and %d more variables\n", length(dropped_vars) - 3))
    }
  }
  
  # Step 3: Keep only good variables plus essential columns
  final_vars <- c(good_vars, available_essential)
  clustering_data <- clustering_data[, final_vars, with = FALSE]
  
  # Step 4: Identify variable types before imputation
  # Better categorical detection
  categorical_vars <- c("ST004D01T", "IMMIG", "SCHLTYPE")
  # Add pattern-based detection
  pattern_categorical <- good_vars[grepl("^ST[0-9].*Q[0-9]|IMMIG|SCHLTYPE", good_vars)]
  categorical_vars <- unique(c(categorical_vars, pattern_categorical))
  available_categorical <- intersect(categorical_vars, good_vars)
  
  continuous_vars <- setdiff(good_vars, available_categorical)
  
  
  # Convert categorical variables to factors
  for (var in available_categorical) {
    if (var %in% names(clustering_data)) {
      clustering_data[[var]] <- as.factor(clustering_data[[var]])
    }
  }
  
  # Step 5: Check if imputation is needed
  remaining_missing <- clustering_data[, lapply(.SD, function(x) sum(is.na(x))), 
                                       .SDcols = good_vars]
  total_missing <- sum(unlist(remaining_missing))
  
  if (total_missing > 0) {
    cat(sprintf("→ Performing MICE imputation for %d remaining missing values...\n", total_missing))
    
    # Prepare data for MICE (only clustering variables)
    mice_data <- clustering_data[, good_vars, with = FALSE]
    
    # Convert to data.frame for mice
    mice_df <- as.data.frame(mice_data)
    
    # EXTRA CHECK: Ensure no haven_labelled columns remain
    for (col_name in names(mice_df)) {
      if (inherits(mice_df[[col_name]], "haven_labelled")) {
        mice_df[[col_name]] <- as.numeric(mice_df[[col_name]])
      }
    }
    
    # Configure MICE methods
    mice_methods <- mice::make.method(mice_df)
    
    # Set appropriate methods for different variable types
    for (var in names(mice_df)) {
      if (is.factor(mice_df[[var]])) {
        # Check number of levels
        n_levels <- length(levels(mice_df[[var]]))
        if (n_levels == 2) {
          mice_methods[var] <- "logreg"  # Binary logistic regression
        } else if (n_levels > 2) {
          mice_methods[var] <- "polyreg"  # Multinomial logistic regression
        }
      } else if (is.numeric(mice_df[[var]])) {
        mice_methods[var] <- "pmm"  # Predictive mean matching for continuous
      }
    }
    
    # Perform MICE imputation with error handling
    tryCatch({
      # Suppress mice output for cleaner console
      mice_result <- mice::mice(mice_df, 
                                m = mice_iterations,           # Number of imputations
                                method = mice_methods,
                                printFlag = FALSE,             # Suppress progress output
                                seed = 42)                     # Reproducibility
      
      # Complete the data using the first imputation
      completed_mice_df <- mice::complete(mice_result, 1)
      
      # Convert back to data.table
      completed_mice_dt <- setDT(completed_mice_df)
      
      # Replace the clustering variables in original data
      for (var in good_vars) {
        clustering_data[[var]] <- completed_mice_dt[[var]]
      }
      
      cat(sprintf("✓ MICE imputation completed successfully\n"))
      
      # Verify no missing values remain
      remaining_after_mice <- clustering_data[, lapply(.SD, function(x) sum(is.na(x))), 
                                              .SDcols = good_vars]
      total_remaining <- sum(unlist(remaining_after_mice))
      
      if (total_remaining > 0) {
        cat(sprintf("⚠ Warning: %d missing values still remain after MICE\n", total_remaining))
        
        # Fallback: Simple imputation for any remaining missings
        for (var in good_vars) {
          if (any(is.na(clustering_data[[var]]))) {
            if (is.factor(clustering_data[[var]])) {
              # Mode imputation for categorical
              mode_val <- names(sort(table(clustering_data[[var]]), decreasing = TRUE))[1]
              clustering_data[is.na(get(var)), (var) := mode_val]
            } else {
              # Median imputation for continuous
              median_val <- median(clustering_data[[var]], na.rm = TRUE)
              clustering_data[is.na(get(var)), (var) := median_val]
            }
          }
        }
        cat("✓ Fallback imputation applied for remaining missing values\n")
      }
      
    }, error = function(e) {
      cat(sprintf("⚠ MICE imputation failed: %s\n", e$message))
      cat("→ Falling back to simple imputation methods...\n")
      
      # Fallback: Simple imputation
      for (var in good_vars) {
        if (any(is.na(clustering_data[[var]]))) {
          if (is.factor(clustering_data[[var]])) {
            # Mode imputation for categorical
            mode_val <- names(sort(table(clustering_data[[var]]), decreasing = TRUE))[1]
            clustering_data[is.na(get(var)), (var) := mode_val]
          } else {
            # Median imputation for continuous
            median_val <- median(clustering_data[[var]], na.rm = TRUE)
            clustering_data[is.na(get(var)), (var) := median_val]
          }
        }
      }
      cat("✓ Fallback imputation completed\n")
    })
    
  } else {
    cat("✓ No missing values detected - no imputation needed\n")
  }
  
  # Step 6: Remove cases with excessive missingness in essential variables
  initial_n <- nrow(clustering_data)
  
  # Check for missing essential variables
  essential_missing <- rowSums(is.na(clustering_data[, available_essential, with = FALSE]))
  clustering_data <- clustering_data[essential_missing == 0]
  
  final_n <- nrow(clustering_data)
  
  if (final_n < initial_n) {
    cat(sprintf("⚠ Removed %d cases with missing essential variables\n", initial_n - final_n))
  }
  
  cat(sprintf("✓ Final dataset: %d students (%.1f%% retention)\n", 
              final_n, 100 * final_n / initial_n))
  
  cat(sprintf("✓ Variable types: %d continuous, %d categorical\n", 
              length(continuous_vars), length(available_categorical)))
  
  # Step 7: Final data quality check
  final_missing_check <- clustering_data[, lapply(.SD, function(x) sum(is.na(x))), 
                                         .SDcols = good_vars]
  total_final_missing <- sum(unlist(final_missing_check))
  
  if (total_final_missing == 0) {
    cat("✓ Data quality check passed: No missing values in clustering variables\n")
  } else {
    cat(sprintf("⚠ Warning: %d missing values still present\n", total_final_missing))
  }
  
  return(list(
    data = clustering_data,
    clustering_vars = good_vars,
    categorical_vars = available_categorical,
    continuous_vars = continuous_vars,
    cycle = cycle_name,
    imputation_summary = list(
      original_missing = total_missing,
      final_missing = total_final_missing,
      variables_dropped = dropped_vars,
      variables_kept = good_vars,
      mice_successful = total_missing > 0 && total_final_missing == 0
    )
  ))
}
# Prepare clustering data for each cycle with MICE imputation
cycle_prepared_data <- list()

for (cycle_name in names(processed_list)) {
  if (!is.null(processed_list[[cycle_name]])) {
    
    cycle_vars <- cycle_variable_analysis[[cycle_name]]$clustering_vars
    
    cycle_prepared_data[[cycle_name]] <- prepare_cycle_clustering_data(
      processed_list[[cycle_name]], 
      cycle_vars, 
      cycle_name,
      missing_threshold = 0.3,  # Drop variables with >30% missing
      mice_iterations = 5       # Number of MICE iterations
    )
  }
}

cat("\n✓ Data preparation with MICE imputation completed for all cycles\n")

# Enhanced Summary with Imputation Details
cat("\n=== CYCLE-SPECIFIC DATASET SUMMARY WITH IMPUTATION DETAILS ===\n")
for (cycle_name in names(cycle_prepared_data)) {
  data_info <- cycle_prepared_data[[cycle_name]]
  imputation_info <- data_info$imputation_summary
  
  cat(sprintf("\nPISA %s:\n", cycle_name))
  cat(sprintf("  Students: %d\n", nrow(data_info$data)))
  cat(sprintf("  Variables: %d total (%d continuous, %d categorical)\n",
              length(data_info$clustering_vars),
              length(data_info$continuous_vars),
              length(data_info$categorical_vars)))
  cat(sprintf("  Variables dropped (>30%% missing): %d\n", length(imputation_info$variables_dropped)))
  
  if (imputation_info$original_missing > 0) {
    cat(sprintf("  MICE imputation: %d missing values → %d missing values\n",
                imputation_info$original_missing, imputation_info$final_missing))
    cat(sprintf("  Imputation success: %s\n", 
                ifelse(imputation_info$mice_successful, "✓ Complete", "⚠ Partial")))
  } else {
    cat("  Imputation: Not needed (no missing values)\n")
  }
}

cat("\n✓ Enhanced cycle-specific data preparation with MICE imputation completed\n")
```

# Enhanced Clustering with ClusterBenchStats

```{r enhanced_clustering, message=FALSE, warning=FALSE}
cat("\n=== ENHANCED CLUSTERING WITH CLUSTERBENCHSTATS ===\n")

# Clean the data before distance calculation
clean_clustering_data <- function(data) {
  clean_data <- as.data.frame(data)
  
  # Convert all haven_labelled columns
  for (col_name in names(clean_data)) {
    if (inherits(clean_data[[col_name]], "haven_labelled")) {
      clean_data[[col_name]] <- as.numeric(clean_data[[col_name]])
    }
  }
  
  # Remove any list columns
  list_cols <- sapply(clean_data, is.list)
  if (any(list_cols)) {
    clean_data <- clean_data[, !list_cols, drop = FALSE]
  }
  
  return(clean_data)
}

# Calculate all three distance matrices using mdist
calculate_cycle_distances_enhanced <- function(cycle_data_info, distance_methods) {
  
  cycle_name <- cycle_data_info$cycle
  cat(sprintf("\n→ Calculating distance matrices for PISA %s...\n", cycle_name))
  
  # Prepare data
  distance_data <- cycle_data_info$data[, cycle_data_info$clustering_vars, with = FALSE]
  distance_data <- na.omit(distance_data)
  
  # Clean the data to remove haven_labelled issues
  distance_data <- clean_clustering_data(distance_data)
  
  # Handle missing values
  for (var in names(distance_data)) {
    if (any(is.na(distance_data[[var]]))) {
      if (is.factor(distance_data[[var]])) {
        mode_val <- names(sort(table(distance_data[[var]]), decreasing = TRUE))[1]
        distance_data[is.na(distance_data[[var]]), var] <- mode_val
      } else {
        median_val <- median(distance_data[[var]], na.rm = TRUE)
        distance_data[is.na(distance_data[[var]]), var] <- median_val
      }
    }
  }
  
  cycle_distance_matrices <- list()
  
  for (method_name in names(distance_methods)) {
    cat(sprintf("  → %s...", method_name))
    
    method_info <- distance_methods[[method_name]]
    
    tryCatch({
      start_time <- Sys.time()
      
      # Use manydist with your original parameters
      dist_matrix <- do.call(manydist::mdist, c(list(x = distance_data), method_info$params))
      
      end_time <- Sys.time()
      calc_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
      
      cycle_distance_matrices[[method_name]] <- list(
        matrix = as.dist(dist_matrix),  # Convert to dist object
        method = method_info$name,
        description = method_info$description,
        calculation_time = calc_time,
        params = method_info$params,
        cycle = cycle_name
      )
      
      cat(sprintf(" %.1fs ✓\n", calc_time))
      
    }, error = function(e) {
      cat(sprintf(" ERROR: %s\n", e$message))
      cat("  → DID NOTHING...\n")
      
    })
  }
  
  return(cycle_distance_matrices)
}

# Education-specific weight development
develop_educational_weights <- function(available_stats, purpose = "policy") {
  
  cat(sprintf("  → Developing %s weights for %d available statistics\n", purpose, length(available_stats)))
  
  # Initialize all weights to a small default value (not zero)
  weights <- rep(0.1, length(available_stats))
  names(weights) <- available_stats
  
  # Print available statistics for debugging
  cat("    Available statistics:", paste(available_stats, collapse = ", "), "\n")
  
  if (purpose == "policy") {
    # Educational Policy Index (Strategic Level)
    for (stat in available_stats) {
      if (grepl("entropy", stat, ignore.case = TRUE)) {
        weights[stat] <- 3  # High entropy weight
      } else if (grepl("within|ave\\.within|homogeneity", stat, ignore.case = TRUE)) {
        weights[stat] <- 1.0  # High within-cluster homogeneity
      } else if (grepl("boot|stability", stat, ignore.case = TRUE)) {
        weights[stat] <- 1.2  # High stability
      } else if (grepl("silhouette|silwidth|asw", stat, ignore.case = TRUE)) {
        weights[stat] <- 0.8  # Medium silhouette
      } else if (grepl("separation|sep|minsep", stat, ignore.case = TRUE)) {
        weights[stat] <- 0.6  # Medium separation
      } else if (grepl("gamma|pearson", stat, ignore.case = TRUE)) {
        weights[stat] <- 0.7  # Structural validity
      }
    }
    
  } else if (purpose == "intervention") {
    # Educational Intervention Index (Operational Level)
    for (stat in available_stats) {
      if (grepl("boot|stability", stat, ignore.case = TRUE)) {
        weights[stat] <- 1.8  # Highest stability
      } else if (grepl("silhouette|silwidth|asw", stat, ignore.case = TRUE)) {
        weights[stat] <- 1.5  # High precision
      } else if (grepl("within|ave\\.within|homogeneity", stat, ignore.case = TRUE)) {
        weights[stat] <- 1.3  # High homogeneity
      } else if (grepl("entropy", stat, ignore.case = TRUE)) {
        weights[stat] <- 1.4  # Actionability
      } else if (grepl("separation|sep|minsep", stat, ignore.case = TRUE)) {
        weights[stat] <- 0.4  # Lower separation priority
      }
    }
    
  } else if (purpose == "equity") {
    # Educational Equity Index (Social Justice Level)
    for (stat in available_stats) {
      if (grepl("separation|sep|dunn|minsep", stat, ignore.case = TRUE)) {
        weights[stat] <- 1.5  # High separation
      } else if (grepl("entropy", stat, ignore.case = TRUE)) {
        weights[stat] <- 1.4  # Balanced representation
      } else if (grepl("silhouette|silwidth|ch|calinski|asw", stat, ignore.case = TRUE)) {
        weights[stat] <- 1.2  # Meaningful differences
      } else if (grepl("boot|stability", stat, ignore.case = TRUE)) {
        weights[stat] <- 0.8  # Medium stability
      }
    }
  }
  
  cat(sprintf("    Weights assigned - non-default weights: %d\n", 
              sum(weights != 0.1)))
  
  return(weights)
}


# FIXED: Extract statistics from clusterbenchstats - handles different structures
extract_clusterbenchstats_matrix <- function(cbs_result) {
  
  # Check if we have the sstat component (standardized statistics)
  if (is.null(cbs_result$sstat)) {
    # Try qstat as fallback
    if (!is.null(cbs_result$qstat)) {
      cat("    Using qstat (non-standardized) instead of sstat\n")
      stat_component <- cbs_result$qstat
    } else {
      cat("    No sstat or qstat component found in results\n")
      return(NULL)
    }
  } else {
    stat_component <- cbs_result$sstat
  }
  
  # Get metadata
  methods <- stat_component$name  # Use 'name' for display names (Ward, Average, etc.)
  if (is.null(methods)) methods <- stat_component$method
  G_values <- stat_component$minG:stat_component$maxG
  stat_names <- stat_component$statistics
  
  # Create empty matrix
  n_rows <- length(methods) * length(G_values)
  stats_matrix <- matrix(NA, nrow = n_rows, ncol = length(stat_names))
  colnames(stats_matrix) <- stat_names
  
  # Row names using the display names
  row_names <- character(n_rows)
  row_idx <- 1
  for (method in methods) {
    for (g in G_values) {
      row_names[row_idx] <- paste(method, g, sep = ".")
      row_idx <- row_idx + 1
    }
  }
  rownames(stats_matrix) <- row_names
  
  # Fill the matrix - NEW LOGIC to handle different structures
  row_idx <- 1
  
  for (method_idx in 1:length(methods)) {
    method_data <- stat_component[[method_idx]]
    
    if (is.list(method_data)) {
      # The structure can vary:
      # - For small G ranges (like 3-6): positions 2-5 have stats for G=3,4,5,6
      # - For larger G ranges (like 8-12): need to map differently
      
      # First, figure out which positions have actual statistics
      stats_positions <- c()
      for (pos in 1:length(method_data)) {
        if (is.list(method_data[[pos]]) && length(method_data[[pos]]) > 0) {
          # Check if this looks like a statistics list
          if (length(intersect(names(method_data[[pos]]), stat_names)) > 0) {
            stats_positions <- c(stats_positions, pos)
          }
        }
      }
      
      # Now map the positions to G values
      if (length(stats_positions) == length(G_values)) {
        # Perfect match - use positions directly
        for (i in seq_along(G_values)) {
          g_stats <- method_data[[stats_positions[i]]]
          
          if (is.list(g_stats) && length(g_stats) > 0) {
            for (col_idx in 1:length(stat_names)) {
              stat_name <- stat_names[col_idx]
              if (stat_name %in% names(g_stats)) {
                value <- g_stats[[stat_name]]
                if (is.numeric(value) && length(value) == 1 && is.finite(value)) {
                  stats_matrix[row_idx, col_idx] <- value
                }
              }
            }
          }
          row_idx <- row_idx + 1
        }
      } else {
        # Try the old logic (position = g_idx + 1)
        for (g_idx in seq_along(G_values)) {
          pos <- g_idx + 1
          if (pos <= length(method_data)) {
            g_stats <- method_data[[pos]]
            
            if (is.list(g_stats) && length(g_stats) > 0) {
              for (col_idx in 1:length(stat_names)) {
                stat_name <- stat_names[col_idx]
                if (stat_name %in% names(g_stats)) {
                  value <- g_stats[[stat_name]]
                  if (is.numeric(value) && length(value) == 1 && is.finite(value)) {
                    stats_matrix[row_idx, col_idx] <- value
                  }
                }
              }
            }
          }
          row_idx <- row_idx + 1
        }
      }
    } else {
      row_idx <- row_idx + length(G_values)
    }
  }
  
  # Remove all-NA rows (if any)
  valid_rows <- rowSums(!is.na(stats_matrix)) > 0
  if (sum(valid_rows) == 0) {
    cat("    ✗ No valid statistics found\n")
    return(NULL)
  }
  
  return(stats_matrix[valid_rows, , drop = FALSE])
}

# Also add a diagnostic function to check what's in the clusterbenchstats result
diagnose_clusterbenchstats <- function(cbs_result, name = "Result") {
  cat(sprintf("\nDiagnosing %s:\n", name))
  
  if (is.null(cbs_result)) {
    cat("  Result is NULL\n")
    return()
  }
  
  cat(sprintf("  Main components: %s\n", paste(names(cbs_result), collapse = ", ")))
  
  if (!is.null(cbs_result$sstat)) {
    cat("  sstat structure:\n")
    cat(sprintf("    Names: %s\n", paste(names(cbs_result$sstat), collapse = ", ")))
    cat(sprintf("    Methods: %s\n", paste(cbs_result$sstat$name, collapse = ", ")))
    cat(sprintf("    G range: %d-%d\n", cbs_result$sstat$minG, cbs_result$sstat$maxG))
    
    # Check first method's data
    if (length(cbs_result$sstat) > 0 && is.list(cbs_result$sstat[[1]])) {
      cat(sprintf("    First method has %d elements\n", length(cbs_result$sstat[[1]])))
      
      # Check if stats are actually there
      for (i in 2:min(4, length(cbs_result$sstat[[1]]))) {
        if (is.list(cbs_result$sstat[[1]][[i]])) {
          cat(sprintf("      Element %d has %d statistics\n", i, length(cbs_result$sstat[[1]][[i]])))
        }
      }
    }
  }
  
  if (!is.null(cbs_result$qstat)) {
    cat("  qstat available (non-standardized stats)\n")
  }
}

find_best_across_distances_FIXED <- function(distance_results, cycle_name) {
  
  all_solutions <- list(
    policy = list(),
    intervention = list(),
    equity = list()
  )
  
  # Collect solutions from all distance methods
  for (dist_method in names(distance_results)) {
    if (!is.null(distance_results[[dist_method]])) {
      
      cat(sprintf("  → Processing %s distance method...\n", dist_method))
      
      # Get available statistics from macro and micro analyses
      macro_analysis <- distance_results[[dist_method]]$macro_analysis
      micro_analysis <- distance_results[[dist_method]]$micro_analysis
      
      # Extract statistics matrices
      macro_stats_matrix <- NULL
      micro_stats_matrix <- NULL
      
      if (!is.null(macro_analysis)) {
        macro_stats_matrix <- extract_clusterbenchstats_matrix(macro_analysis)
      }
      
      if (!is.null(micro_analysis)) {
        micro_stats_matrix <- extract_clusterbenchstats_matrix(micro_analysis)
      }
      
      # Process macro statistics for policy and equity
      if (!is.null(macro_stats_matrix)) {
        # Apply education-specific weights
        available_stats <- colnames(macro_stats_matrix)
        policy_weights <- develop_educational_weights(available_stats, "policy")
        equity_weights <- develop_educational_weights(available_stats, "equity")
        
        # Calculate weighted scores
        policy_scores <- apply(macro_stats_matrix, 1, function(row) {
          if (all(is.na(row))) return(NA)
          row[!is.finite(row)] <- NA
          valid_indices <- !is.na(row)
          if (sum(valid_indices) == 0) return(NA)
          weighted.mean(row[valid_indices], policy_weights[valid_indices], na.rm = TRUE)
        })
        
        equity_scores <- apply(macro_stats_matrix, 1, function(row) {
          if (all(is.na(row))) return(NA)
          row[!is.finite(row)] <- NA
          valid_indices <- !is.na(row)
          if (sum(valid_indices) == 0) return(NA)
          weighted.mean(row[valid_indices], equity_weights[valid_indices], na.rm = TRUE)
        })
        
        # Store best solutions
        if (length(policy_scores) > 0 && any(!is.na(policy_scores))) {
          best_policy_idx <- which.max(policy_scores)
          best_policy_name <- names(policy_scores)[best_policy_idx]
          
          all_solutions$policy[[dist_method]] <- list(
            score = policy_scores[best_policy_idx],
            solution = best_policy_name,
            distance_method = dist_method,
            statistics = macro_stats_matrix[best_policy_name, ]
          )
        }
        
        if (length(equity_scores) > 0 && any(!is.na(equity_scores))) {
          best_equity_idx <- which.max(equity_scores)
          best_equity_name <- names(equity_scores)[best_equity_idx]
          
          all_solutions$equity[[dist_method]] <- list(
            score = equity_scores[best_equity_idx],
            solution = best_equity_name,
            distance_method = dist_method,
            statistics = macro_stats_matrix[best_equity_name, ]
          )
        }
      }
      
      # Process micro analysis for intervention
      if (!is.null(micro_stats_matrix)) {
        available_stats <- colnames(micro_stats_matrix)
        intervention_weights <- develop_educational_weights(available_stats, "intervention")
        
        intervention_scores <- apply(micro_stats_matrix, 1, function(row) {
          if (all(is.na(row))) return(NA)
          row[!is.finite(row)] <- NA
          valid_indices <- !is.na(row)
          if (sum(valid_indices) == 0) return(NA)
          weighted.mean(row[valid_indices], intervention_weights[valid_indices], na.rm = TRUE)
        })
        
        if (length(intervention_scores) > 0 && any(!is.na(intervention_scores))) {
          best_intervention_idx <- which.max(intervention_scores)
          best_intervention_name <- names(intervention_scores)[best_intervention_idx]
          
          all_solutions$intervention[[dist_method]] <- list(
            score = intervention_scores[best_intervention_idx],
            solution = best_intervention_name,
            distance_method = dist_method,
            statistics = micro_stats_matrix[best_intervention_name, ]
          )
        }
      }
    }
  }
  
  # Find overall best for each purpose
  best_overall <- list()
  
  for (purpose in c("policy", "intervention", "equity")) {
    if (length(all_solutions[[purpose]]) > 0) {
      scores <- sapply(all_solutions[[purpose]], function(x) x$score)
      best_dist_method <- names(scores)[which.max(scores)]
      best_overall[[purpose]] <- all_solutions[[purpose]][[best_dist_method]]
      
      cat(sprintf("  ✓ Best %s solution: %s using %s (Score: %.3f)\n", 
                  purpose, best_overall[[purpose]]$solution, 
                  best_overall[[purpose]]$distance_method, 
                  best_overall[[purpose]]$score))
    } else {
      cat(sprintf("  ✗ No %s solution found\n", purpose))
    }
  }
  
  # Return the results - THIS WAS THE MISSING PART
  return(best_overall)
}

# The issue is likely in the find_best_across_distances_WORKING function
# Let's create a simplified version that will work reliably
find_best_solutions_simplified <- function(distance_results, cycle_name) {
  best_solutions <- list(policy = NULL, intervention = NULL, equity = NULL)
  
  # For each distance method, extract the best solutions
  for (dist_method in names(distance_results)) {
    dist_result <- distance_results[[dist_method]]
    
    # Extract macro statistics if available
    if (!is.null(dist_result$macro_analysis)) {
      macro_stats <- extract_clusterbenchstats_matrix(dist_result$macro_analysis)
      
      if (!is.null(macro_stats)) {
        # Calculate simple average score for policy and equity
        policy_scores <- rowMeans(macro_stats, na.rm = TRUE)
        equity_scores <- rowMeans(macro_stats[, c("minsep", "asw", "dindex", "entropy")], na.rm = TRUE)
        
        # Find best solutions
        best_policy <- names(which.max(policy_scores))
        best_equity <- names(which.max(equity_scores))
        
        # Store if better than current best
        if (is.null(best_solutions$policy) || 
            policy_scores[best_policy] > best_solutions$policy$score) {
          best_solutions$policy <- list(
            solution = best_policy,
            score = policy_scores[best_policy],
            distance_method = dist_method
          )
        }
        
        if (is.null(best_solutions$equity) || 
            equity_scores[best_equity] > best_solutions$equity$score) {
          best_solutions$equity <- list(
            solution = best_equity,
            score = equity_scores[best_equity],
            distance_method = dist_method
          )
        }
      }
    }
    
    # Extract micro statistics if available
    if (!is.null(dist_result$micro_analysis)) {
      micro_stats <- extract_clusterbenchstats_matrix(dist_result$micro_analysis)
      
      if (!is.null(micro_stats)) {
        # Calculate simple average score for intervention
        intervention_scores <- rowMeans(micro_stats, na.rm = TRUE)
        best_intervention <- names(which.max(intervention_scores))
        
        # Store if better than current best
        if (is.null(best_solutions$intervention) || 
            intervention_scores[best_intervention] > best_solutions$intervention$score) {
          best_solutions$intervention <- list(
            solution = best_intervention,
            score = intervention_scores[best_intervention],
            distance_method = dist_method
          )
        }
      }
    }
  }
  
  return(best_solutions)
}

# Enhanced clustering function using clusterbenchstats with multiple distances
perform_educational_clustering_official <- function(cycle_data, cycle_name) {
  
  cat(sprintf("\n=== CLUSTERBENCHSTATS ANALYSIS: PISA %s ===\n", cycle_name))
  
  # Step 1: Calculate all three distance matrices
  cycle_distances <- calculate_cycle_distances_enhanced(cycle_data, DISTANCE_METHODS)
  
  # Step 2: Configure clustering methods (ALL DIST-COMPATIBLE)
  # IMPORTANT: Define these BEFORE the for loop
  clustermethod <- c("disthclustCBI", "disthclustCBI", "disthclustCBI", "pamkCBI")
  
  # Step 3: Configure method parameters
  clustermethodpars <- list()
  clustermethodpars[[1]] <- list(method = "ward.D2")
 # clustermethodpars[[2]] <- list(method = "average")
  clustermethodpars[[2]] <- list(method = "ward.D2")
  clustermethodpars[[3]] <- list(method = "ward.D2")
  clustermethodpars[[4]] <- list(diss = TRUE, usepam = TRUE)  # PAM with dissimilarity matrix
  
  # Step 4: Method identification
  methodnames <- c("Ward", "Complete", "Complete", "PAM")
  
  
  # Step 5: Distance method specification
  distmethod <- rep(TRUE, length(clustermethod))
  ncinput <- rep(TRUE, length(clustermethod))
  
  # Step 6: Define cluster ranges
  G_macro <- CYCLE_CLUSTERING_PARAMS$macro$k_min:CYCLE_CLUSTERING_PARAMS$macro$k_max
  G_micro <- CYCLE_CLUSTERING_PARAMS$micro$k_min:CYCLE_CLUSTERING_PARAMS$micro$k_max
  
  # Step 7: Run clustering analysis for each distance method
  distance_results <- list()
  
  for (dist_method_name in names(cycle_distances)) {
    if (!is.null(cycle_distances[[dist_method_name]])) {
      
      cat(sprintf("\n→ Running clustering with %s distance...\n", dist_method_name))
      
      dist_matrix <- cycle_distances[[dist_method_name]]$matrix
      
      # Initialize results for this distance method
      cbs_macro <- NULL
      cbs_micro <- NULL
      
      # Run macro-level analysis
      cat("  → Macro-level clustering...\n")
      tryCatch({
        # All variables are now in scope
        cbs_macro <- clusterbenchstats(
          data = dist_matrix,
          G = G_macro,
          diss = TRUE,
          scaling = FALSE,
          clustermethod = clustermethod,
          methodnames = methodnames,
          distmethod = distmethod,
          ncinput = ncinput,
          clustermethodpars = clustermethodpars,
          npstats = FALSE,
          useboot = FALSE,
          trace = FALSE,
          useallmethods = FALSE,
          useallg = FALSE,
          nnruns = 2,
          kmruns = 2,
          fnruns = 2,
          avenruns = 2
        )
        
        cat("    ✓ Macro clustering completed\n")
        
      }, error = function(e) {
        cat(sprintf("    ✗ Macro clustering failed: %s\n", e$message))
        # cbs_macro already initialized as NULL
      })
      
      # Run micro-level analysis
      cat("  → Micro-level clustering...\n")
      tryCatch({
        cbs_micro <- clusterbenchstats(
          data = dist_matrix,
          G = G_micro,
          diss = TRUE,
          scaling = FALSE,
          clustermethod = clustermethod,
          methodnames = methodnames,
          distmethod = distmethod,
          ncinput = ncinput,
          clustermethodpars = clustermethodpars,
          npstats = FALSE,
          useboot = FALSE,
          trace = FALSE,
          useallmethods = FALSE,
          useallg = FALSE,
          nnruns = 2,
          kmruns = 2,
          fnruns = 2,
          avenruns = 2
        )
        
        cat("    ✓ Micro clustering completed\n")
        
      }, error = function(e) {
        cat(sprintf("    ✗ Micro clustering failed: %s\n", e$message))
        # cbs_micro already initialized as NULL
      })
      
      # Store results for this distance method
      distance_results[[dist_method_name]] <- list(
        macro_analysis = cbs_macro,
        micro_analysis = cbs_micro,
        distance_info = cycle_distances[[dist_method_name]]
      )
    }
  }
  
  # Step 8: Find best solutions across all distance methods
  cat("\n→ Finding optimal solutions across all distance methods...\n")
  
  # Replace the call to find_best_across_distances_WORKING with:
  best_solutions <- find_best_solutions_simplified(distance_results, cycle_name)
  
  return(list(
    cycle = cycle_name,
    data_info = list(
      n_students = nrow(na.omit(cycle_data$data[, cycle_data$clustering_vars, with = FALSE])),
      n_variables = length(cycle_data$clustering_vars),
      continuous_vars = cycle_data$continuous_vars,
      categorical_vars = cycle_data$categorical_vars
    ),
    distance_results = distance_results,
    optimal_solutions = best_solutions,
    distance_matrices = cycle_distances
  ))
}

cat("✓ Enhanced clustering functions with clusterbenchstats loaded\n")
```

# Comprehensive Clustering Analysis

```{r comprehensive_clustering_analysis, message=FALSE, warning=FALSE}
cat("\n=== COMPREHENSIVE CLUSTERING ANALYSIS WITH CLUSTERBENCHSTATS ===\n")

# Apply enhanced clustering to all cycles
enhanced_clustering_results <- list()

for (cycle_name in names(cycle_prepared_data)) {
  if (!is.null(cycle_prepared_data[[cycle_name]])) {
    
    enhanced_clustering_results[[cycle_name]] <- perform_educational_clustering_official(
      cycle_prepared_data[[cycle_name]], 
      cycle_name
    )
  }
}

cat("\n✓ Enhanced clustering analysis completed for all cycles\n")

# Display results
cat("\n=== FINAL OPTIMAL CLUSTERING RESULTS ===\n")

for (cycle_name in names(enhanced_clustering_results)) {
  result <- enhanced_clustering_results[[cycle_name]]
  
  cat(sprintf("\n--- PISA %s RESULTS ---\n", cycle_name))
  cat(sprintf("Students analyzed: %d\n", result$data_info$n_students))
  cat(sprintf("Variables used: %d\n", result$data_info$n_variables))
  
  # Display optimal solutions
  for (purpose in c("policy", "intervention", "equity")) {
    if (!is.null(result$optimal_solutions[[purpose]])) {
      sol <- result$optimal_solutions[[purpose]]
      cat(sprintf("%s: %s using %s distance (Score: %.3f)\n", 
                  toupper(purpose), sol$solution, sol$distance_method, sol$score))
    } else {
      cat(sprintf("%s: No solution found\n", toupper(purpose)))
    }
  }
}
```

```{r working_clustering_analysis, echo=TRUE}
# REVISED: Extract clusters for ALL three purposes
extract_all_clusters_from_solutions <- function(enhanced_clustering_results) {
  
  all_cluster_assignments <- list(
    policy = list(),
    intervention = list(),
    equity = list()
  )
  
  for (cycle_name in names(enhanced_clustering_results)) {
    result <- enhanced_clustering_results[[cycle_name]]
    
    cat(sprintf("\n→ Extracting all cluster types for PISA %s...\n", cycle_name))
    
    # Extract clusters for each purpose
    for (purpose in c("policy", "intervention", "equity")) {
      
      if (!is.null(result$optimal_solutions[[purpose]])) {
        
        sol <- result$optimal_solutions[[purpose]]
        dist_method <- sol$distance_method
        
        # Get the distance matrix
        dist_matrix <- result$distance_matrices[[dist_method]]$matrix
        
        # Parse the solution to get method and k
        solution_parts <- strsplit(sol$solution, "\\.")[[1]]
        method_name <- solution_parts[1]
        k_value <- as.numeric(solution_parts[2])
        
        # Perform the clustering based on method
        clusters <- NULL
        
        if (method_name == "Ward") {
          hc <- hclust(dist_matrix, method = "ward.D2")
          clusters <- cutree(hc, k = k_value)
        } else if (method_name == "Average") {
          hc <- hclust(dist_matrix, method = "complete")
          clusters <- cutree(hc, k = k_value)
        } else if (method_name == "Complete") {
          hc <- hclust(dist_matrix, method = "complete")
          clusters <- cutree(hc, k = k_value)
        } else if (method_name == "PAM") {
          clusters <- pam(dist_matrix, k = k_value, diss = TRUE)$clustering
        } else {
          # Default to PAM
          clusters <- pam(dist_matrix, k = k_value, diss = TRUE)$clustering
        }
        
        # Store the cluster assignment
        all_cluster_assignments[[purpose]][[cycle_name]] <- list(
          clusters = clusters,
          method = method_name,
          k = k_value,
          distance_method = dist_method,
          purpose = purpose,
          cycle = cycle_name,
          solution_name = sol$solution,
          score = sol$score
        )
        
        cat(sprintf("  %s: %d clusters using %s method with %s distance (Score: %.3f)\n",
                    toupper(purpose), k_value, method_name, dist_method, sol$score))
        
      } else {
        cat(sprintf("  %s: No solution available\n", toupper(purpose)))
      }
    }
  }
  
  return(all_cluster_assignments)
}

# REVISED: Enhanced cluster summary function
summarize_all_clusters <- function(all_cluster_assignments) {
  
  cat("\n=== COMPREHENSIVE CLUSTER ASSIGNMENT SUMMARY ===\n")
  
  for (purpose in names(all_cluster_assignments)) {
    
    cat(sprintf("\n--- %s CLUSTERS ---\n", toupper(purpose)))
    
    purpose_clusters <- all_cluster_assignments[[purpose]]
    
    if (length(purpose_clusters) == 0) {
      cat("No clusters extracted for this purpose\n")
      next
    }
    
    for (cycle_name in names(purpose_clusters)) {
      cluster_info <- purpose_clusters[[cycle_name]]
      
      cat(sprintf("PISA %s:\n", cycle_name))
      cat(sprintf("  Solution: %s\n", cluster_info$solution_name))
      cat(sprintf("  Method: %s linkage\n", cluster_info$method))
      cat(sprintf("  Distance: %s\n", cluster_info$distance_method))
      cat(sprintf("  Clusters: %d\n", cluster_info$k))
      cat(sprintf("  Score: %.3f\n", cluster_info$score))
      cat(sprintf("  Students: %d\n", length(cluster_info$clusters)))
      
      # Show cluster distribution
      cluster_table <- table(cluster_info$clusters)
      cat("  Distribution:", paste(names(cluster_table), "=", cluster_table, collapse = ", "), "\n")
      
      cat("\n")
    }
  }
}

# REVISED: Function to add cluster assignments to original data
add_clusters_to_data <- function(cycle_prepared_data, all_cluster_assignments) {
  
  enhanced_data <- list()
  
  for (cycle_name in names(cycle_prepared_data)) {
    
    cat(sprintf("→ Adding cluster assignments to PISA %s data...\n", cycle_name))
    
    # Start with the original prepared data
    cycle_data <- cycle_prepared_data[[cycle_name]]$data
    enhanced_cycle_data <- copy(cycle_data)
    
    # Add cluster assignments for each purpose
    for (purpose in names(all_cluster_assignments)) {
      
      if (cycle_name %in% names(all_cluster_assignments[[purpose]])) {
        
        cluster_info <- all_cluster_assignments[[purpose]][[cycle_name]]
        clusters <- cluster_info$clusters
        
        # Create column name
        cluster_col_name <- paste0("CLUSTER_", toupper(purpose))
        
        # Add clusters (need to handle potential row mismatches)
        if (length(clusters) == nrow(enhanced_cycle_data)) {
          enhanced_cycle_data[[cluster_col_name]] <- clusters
          cat(sprintf("  ✓ Added %s clusters (%d groups)\n", 
                      purpose, cluster_info$k))
        } else {
          cat(sprintf("  ⚠ %s cluster count mismatch: %d clusters vs %d rows\n",
                      purpose, length(clusters), nrow(enhanced_cycle_data)))
        }
      }
    }
    
    enhanced_data[[cycle_name]] <- list(
      data = enhanced_cycle_data,
      clustering_vars = cycle_prepared_data[[cycle_name]]$clustering_vars,
      categorical_vars = cycle_prepared_data[[cycle_name]]$categorical_vars,
      continuous_vars = cycle_prepared_data[[cycle_name]]$continuous_vars,
      cycle = cycle_name,
      cluster_info = all_cluster_assignments
    )
  }
  
  return(enhanced_data)
}

# REVISED: Cross-purpose cluster comparison
compare_cluster_purposes <- function(all_cluster_assignments) {
  
  cat("\n=== CROSS-PURPOSE CLUSTER COMPARISON ===\n")
  
  for (cycle_name in names(enhanced_clustering_results)) {
    
    cat(sprintf("\n--- PISA %s COMPARISON ---\n", cycle_name))
    
    # Check if all three purposes have solutions
    purposes_available <- c()
    for (purpose in c("policy", "intervention", "equity")) {
      if (cycle_name %in% names(all_cluster_assignments[[purpose]])) {
        purposes_available <- c(purposes_available, purpose)
      }
    }
    
    if (length(purposes_available) == 0) {
      cat("No cluster solutions available\n")
      next
    }
    
    cat(sprintf("Available solutions: %s\n", paste(purposes_available, collapse = ", ")))
    
    # Create comparison table
    comparison_df <- data.frame(
      Purpose = character(0),
      Solution = character(0),
      Method = character(0),
      Distance = character(0),
      K = numeric(0),
      Score = numeric(0),
      stringsAsFactors = FALSE
    )
    
    for (purpose in purposes_available) {
      cluster_info <- all_cluster_assignments[[purpose]][[cycle_name]]
      
      comparison_df <- rbind(comparison_df, data.frame(
        Purpose = toupper(purpose),
        Solution = cluster_info$solution_name,
        Method = cluster_info$method,
        Distance = cluster_info$distance_method,
        K = cluster_info$k,
        Score = round(cluster_info$score, 3),
        stringsAsFactors = FALSE
      ))
    }
    
    print(comparison_df)
    
    # If we have multiple solutions, check cluster agreement
    if (length(purposes_available) > 1) {
      
      cat("\nCluster agreement analysis:\n")
      
      # Get cluster assignments for available purposes
      cluster_matrices <- list()
      for (purpose in purposes_available) {
        cluster_matrices[[purpose]] <- all_cluster_assignments[[purpose]][[cycle_name]]$clusters
      }
      
      # Calculate pairwise agreement (Adjusted Rand Index)
      if (length(cluster_matrices) >= 2) {
        library(mclust)
        
        purpose_pairs <- combn(purposes_available, 2, simplify = FALSE)
        
        for (pair in purpose_pairs) {
          purpose1 <- pair[1]
          purpose2 <- pair[2]
          
          clusters1 <- cluster_matrices[[purpose1]]
          clusters2 <- cluster_matrices[[purpose2]]
          
          if (length(clusters1) == length(clusters2)) {
            ari <- adjustedRandIndex(clusters1, clusters2)
            cat(sprintf("  %s vs %s: ARI = %.3f\n", 
                        toupper(purpose1), toupper(purpose2), ari))
          }
        }
      }
    }
  }
}

# EXECUTE THE REVISED EXTRACTION
cat("\n=== EXTRACTING ALL CLUSTER TYPES ===\n")

# Extract all cluster types
all_final_clusters <- extract_all_clusters_from_solutions(enhanced_clustering_results)

# Summarize all clusters
summarize_all_clusters(all_final_clusters)

# Compare across purposes
compare_cluster_purposes(all_final_clusters)

# Add clusters to original data
enhanced_data_with_clusters <- add_clusters_to_data(cycle_prepared_data, all_final_clusters)

cat("\n✓ All cluster extractions completed successfully!\n")
cat("✓ Data enhanced with policy, intervention, and equity cluster assignments\n")

# OPTIONAL: Quick verification of the enhanced data
cat("\n=== DATA ENHANCEMENT VERIFICATION ===\n")

for (cycle_name in names(enhanced_data_with_clusters)) {
  enhanced_info <- enhanced_data_with_clusters[[cycle_name]]
  data_cols <- names(enhanced_info$data)
  
  cluster_cols <- data_cols[grepl("^CLUSTER_", data_cols)]
  
  cat(sprintf("PISA %s: %d total columns, %d cluster columns (%s)\n",
              cycle_name, length(data_cols), length(cluster_cols),
              paste(cluster_cols, collapse = ", ")))
}

cat("\n✓ Ready for comprehensive resilience profile analysis!\n")
```




```{r stability_filtering, message=FALSE, warning=FALSE}
cat("\n=== STABILITY-FILTERED CLUSTER SELECTION ===\n")

# COMPREHENSIVE BOOTSTRAP STABILITY TESTING
library(fpc)
library(cluster)

# Enhanced stability testing that filters solutions
perform_stability_filtered_clustering <- function(enhanced_clustering_results, stability_threshold = 0.01) {
  
  stability_filtered_results <- list()
  
  for (cycle_name in names(enhanced_clustering_results)) {
    result <- enhanced_clustering_results[[cycle_name]]
    
    cat(sprintf("\n→ Stability filtering for PISA %s...\n", cycle_name))
    
    filtered_solutions <- list(policy = NULL, intervention = NULL, equity = NULL)
    
    for (purpose in c("policy", "intervention", "equity")) {
      if (!is.null(result$optimal_solutions[[purpose]])) {
        
        sol <- result$optimal_solutions[[purpose]]
        dist_method <- sol$distance_method
        dist_matrix <- result$distance_matrices[[dist_method]]$matrix
        
        # Parse solution
        solution_parts <- strsplit(sol$solution, "\\.")[[1]]
        method_name <- solution_parts[1]
        k_value <- as.numeric(solution_parts[2])
        
        cat(sprintf("  Testing %s stability: %s with k=%d...\n", purpose, method_name, k_value))
        
        # Create clustering function
        clust_function <- create_stability_clustering_function(method_name, k_value)
        
        # Run bootstrap stability test
        tryCatch({
          boot_result <- clusterboot(
            data = dist_matrix,
            B = 50,  # Number of bootstrap samples
            distances = TRUE,
            bootmethod = "boot",
            clustermethod = clust_function,
            k = k_value,
            seed = 42
          )
          
          # Calculate overall stability
          overall_stability <- mean(boot_result$bootmean, na.rm = TRUE)
          stable_clusters <- sum(boot_result$bootmean > stability_threshold, na.rm = TRUE)
          
          cat(sprintf("    Overall stability: %.3f\n", overall_stability))
          cat(sprintf("    Stable clusters: %d/%d\n", stable_clusters, k_value))
          
          # Apply stability filter
          if (overall_stability >= stability_threshold) {
            # Stability-weighted score
            stability_weight <- overall_stability
            original_score <- sol$score
            
            # HYBRID SCORE: Combine stability and original composite score
            hybrid_score <- (stability_weight^2) * original_score
            
            filtered_solutions[[purpose]] <- list(
              solution = sol$solution,
              score = original_score,
              stability_score = overall_stability,
              hybrid_score = hybrid_score,
              stable_clusters = stable_clusters,
              distance_method = dist_method,
              cluster_stabilities = boot_result$bootmean,
              passed_filter = TRUE
            )
            
            cat(sprintf("    ✓ PASSED stability filter (Hybrid score: %.3f)\n", hybrid_score))
            
          } else {
            cat(sprintf("    ✗ FAILED stability filter (%.3f < %.3f)\n", 
                        overall_stability, stability_threshold))
            
            # Still store but mark as failed
            filtered_solutions[[purpose]] <- list(
              solution = sol$solution,
              score = sol$score,
              stability_score = overall_stability,
              hybrid_score = 0,  # Zero out failed solutions
              stable_clusters = stable_clusters,
              distance_method = dist_method,
              passed_filter = FALSE
            )
          }
          
        }, error = function(e) {
          cat(sprintf("    ✗ Stability test failed: %s\n", e$message))
          filtered_solutions[[purpose]] <- NULL
        })
      }
    }
    
    stability_filtered_results[[cycle_name]] <- list(
      original_results = result,
      stability_filtered_solutions = filtered_solutions,
      stability_threshold = stability_threshold
    )
  }
  
  return(stability_filtered_results)
}

# Helper function for clustering methods
create_stability_clustering_function <- function(method_name, k_value) {
  if (method_name == "Ward") {
    return(function(x, k = k_value) {
      if (inherits(x, "dist")) {
        hc <- hclust(x, method = "ward.D2")
        clusters <- cutree(hc, k = k)
        print(clusters)
      } else {
        d <- dist(x)
        hc <- hclust(d, method = "ward.D2")
        clusters <- cutree(hc, k = k)
      }
      return(list(result = clusters, nc = k))
    })
  } else if (method_name == "Average") {
    return(function(x, k = k_value) {
      if (inherits(x, "dist")) {
        hc <- hclust(x, method = "complete")
        clusters <- cutree(hc, k = k)
      } else {
        d <- dist(x)
        hc <- hclust(d, method = "complete")
        clusters <- cutree(hc, k = k)
      }
      return(list(result = clusters, nc = k))
    })
  } else if (method_name == "Complete") {
    return(function(x, k = k_value) {
      if (inherits(x, "dist")) {
        hc <- hclust(x, method = "complete")
        clusters <- cutree(hc, k = k)
      } else {
        d <- dist(x)
        hc <- hclust(d, method = "complete")
        clusters <- cutree(hc, k = k)
      }
      return(list(result = clusters, nc = k))
    })
  } else if (method_name == "PAM") {
    return(function(x, k = k_value) {
      if (inherits(x, "dist")) {
        pam_result <- pam(x, k = k, diss = TRUE)
        clusters <- pam_result$clustering
      } else {
        pam_result <- pam(x, k = k, diss = FALSE)
        clusters <- pam_result$clustering
      }
      return(list(result = clusters, nc = k))
    })
  }
}

# Apply stability filtering
stability_filtered_results <- perform_stability_filtered_clustering(
  enhanced_clustering_results, 
  stability_threshold = 0.01 # Adjust as needed
)

# Update your cluster extraction to use stability-filtered results
extract_stability_filtered_clusters <- function(stability_filtered_results) {
  
  all_cluster_assignments <- list(
    policy = list(),
    intervention = list(),
    equity = list()
  )
  
  for (cycle_name in names(stability_filtered_results)) {
    result <- stability_filtered_results[[cycle_name]]
    
    cat(sprintf("\n→ Extracting stability-filtered clusters for PISA %s...\n", cycle_name))
    
    for (purpose in c("policy", "intervention", "equity")) {
      
      filtered_sol <- result$stability_filtered_solutions[[purpose]]
      
      if (!is.null(filtered_sol) && filtered_sol$passed_filter) {
        
        # Get original distance matrix
        original_result <- result$original_results
        dist_method <- filtered_sol$distance_method
        dist_matrix <- original_result$distance_matrices[[dist_method]]$matrix
        
        # Parse solution
        solution_parts <- strsplit(filtered_sol$solution, "\\.")[[1]]
        method_name <- solution_parts[1]
        k_value <- as.numeric(solution_parts[2])
        
        # Perform clustering
        if (method_name == "Ward") {
          hc <- hclust(dist_matrix, method = "ward.D2")
          clusters <- cutree(hc, k = k_value)
        } else if (method_name == "Average") {
          hc <- hclust(dist_matrix, method = "complete")
          clusters <- cutree(hc, k = k_value)
        } else if (method_name == "Complete") {
          hc <- hclust(dist_matrix, method = "complete")
          clusters <- cutree(hc, k = k_value)
        } else if (method_name == "PAM") {
          clusters <- pam(dist_matrix, k = k_value, diss = TRUE)$clustering
        }
        
        all_cluster_assignments[[purpose]][[cycle_name]] <- list(
          clusters = clusters,
          method = method_name,
          k = k_value,
          distance_method = dist_method,
          purpose = purpose,
          cycle = cycle_name,
          solution_name = filtered_sol$solution,
          original_score = filtered_sol$score,
          stability_score = filtered_sol$stability_score,
          hybrid_score = filtered_sol$hybrid_score,
          stable_clusters = filtered_sol$stable_clusters
        )
        
        cat(sprintf("  ✓ %s: %d clusters (Stability: %.3f, Hybrid: %.3f)\n",
                    toupper(purpose), k_value, 
                    filtered_sol$stability_score, filtered_sol$hybrid_score))
        
      } else {
        if (!is.null(filtered_sol)) {
          cat(sprintf("  ✗ %s: Failed stability filter (%.3f)\n",
                      toupper(purpose), filtered_sol$stability_score))
        } else {
          cat(sprintf("  ✗ %s: No solution available\n", toupper(purpose)))
        }
      }
    }
  }
  
  return(all_cluster_assignments)
}

# Extract stability-filtered clusters
stability_filtered_clusters <- extract_stability_filtered_clusters(stability_filtered_results)

cat("\n✓ Stability filtering completed!\n")
```

# Cross-Cycle Comparison

```{r cross_cycle_comparison, message=FALSE, warning=FALSE}
cat("\n=== ENHANCED CROSS-CYCLE COMPARISON ===\n")

# Use the enhanced results for comparison
if (exists("enhanced_clustering_results")) {
  cycle_optimal_results <- enhanced_clustering_results
  
  # Simple comparison of results across cycles
  cat("Cross-cycle optimal clustering summary:\n")
  
  for (cycle_name in names(cycle_optimal_results)) {
    result <- cycle_optimal_results[[cycle_name]]
    
    cat(sprintf("\nPISA %s:\n", cycle_name))
    cat(sprintf("  Students: %d\n", result$data_info$n_students))
    cat(sprintf("  Variables: %d\n", result$data_info$n_variables))
    
    # Distance method performance
    cat("  Distance method performance:\n")
    for (dist_method in names(result$distance_matrices)) {
      if (!is.null(result$distance_matrices[[dist_method]])) {
        cat(sprintf("    %s: ✓ Calculated (%.1fs)\n", 
                    dist_method, 
                    result$distance_matrices[[dist_method]]$calculation_time))
      }
    }
    
    # Show which methods performed best for each index
    if (!is.null(result$optimal_solutions$policy)) {
      cat(sprintf("  Best policy solution: %s\n", result$optimal_solutions$policy$distance_method))
    }
    if (!is.null(result$optimal_solutions$intervention)) {
      cat(sprintf("  Best intervention solution: %s\n", result$optimal_solutions$intervention$distance_method))
    }
    if (!is.null(result$optimal_solutions$equity)) {
      cat(sprintf("  Best equity solution: %s\n", result$optimal_solutions$equity$distance_method))
    }
  }
  
  # Cross-cycle distance method consistency
  cat("\n=== DISTANCE METHOD CONSISTENCY ACROSS CYCLES ===\n")
  
  policy_methods <- c()
  intervention_methods <- c()
  equity_methods <- c()
  
  for (cycle_name in names(cycle_optimal_results)) {
    result <- cycle_optimal_results[[cycle_name]]
    
    if (!is.null(result$optimal_solutions$policy)) {
      policy_methods <- c(policy_methods, result$optimal_solutions$policy$distance_method)
    }
    if (!is.null(result$optimal_solutions$intervention)) {
      intervention_methods <- c(intervention_methods, result$optimal_solutions$intervention$distance_method)
    }
    if (!is.null(result$optimal_solutions$equity)) {
      equity_methods <- c(equity_methods, result$optimal_solutions$equity$distance_method)
    }
  }
  
  cat("Policy index preferred distance methods:\n")
  if (length(policy_methods) > 0) {
    policy_table <- table(policy_methods)
    for (i in 1:length(policy_table)) {
      cat(sprintf("  %s: %d/%d cycles\n", names(policy_table)[i], policy_table[i], length(policy_methods)))
    }
  }
  
  cat("Intervention index preferred distance methods:\n")
  if (length(intervention_methods) > 0) {
    intervention_table <- table(intervention_methods)
    for (i in 1:length(intervention_table)) {
      cat(sprintf("  %s: %d/%d cycles\n", names(intervention_table)[i], intervention_table[i], length(intervention_methods)))
    }
  }
  
  cat("Equity index preferred distance methods:\n")
  if (length(equity_methods) > 0) {
    equity_table <- table(equity_methods)
    for (i in 1:length(equity_table)) {
      cat(sprintf("  %s: %d/%d cycles\n", names(equity_table)[i], equity_table[i], length(equity_methods)))
    }
  }
}

cat("\n✓ Enhanced cross-cycle comparison completed\n")
```

```{r cluster_interpretation}
# First, let's create a function to generate comprehensive cluster profiles
# Fix the cluster profile creation function
create_detailed_cluster_profiles <- function(cycle_data, cluster_assignments, cycle_name) {
  cat(sprintf("\n=== CREATING DETAILED CLUSTER PROFILES FOR PISA %s ===\n", cycle_name))
  
  # Get the data
  data <- copy(cycle_data$data)  # Use copy to avoid modifying original data
  clustering_vars <- cycle_data$clustering_vars
  
  # Add cluster assignments
  data[, CLUSTER := cluster_assignments$clusters]
  
  # Define comprehensive profiling variables across domains
  profile_vars <- list(
    achievement = c("MATH_AVG", "READ_AVG", "SCIE_AVG"),
    socioeconomic = c("ESCS", "HISEI", "PAREDINT", "HOMEPOS", "ICTRES"),
    demographics = c("ST004D01T", "IMMIG"),
    motivation = c("ANXMAT", "MATHEFF", "MATHMOT", "INTMAT", "RESILIENCE"),
    social_emotional = c("BELONG", "TEACHSUP", "PEERREL", "FAMSUP"),
    behavior = c("HOMWRK", "OUTHOURS", "PERSEV", "TRUANCY"),
    outcome = c("RESILIENT")
  )
  
  # Keep only available variables
  available_vars <- unlist(lapply(profile_vars, function(vars) {
    intersect(vars, names(data))
  }))
  
  # Create summary statistics for each cluster
  cluster_summaries <- list()
  
  for (cluster_id in unique(data$CLUSTER)) {
    cluster_data <- data[CLUSTER == cluster_id, ]
    
    # Calculate means for numeric variables using ..available_vars
    numeric_summary <- cluster_data[, lapply(.SD, mean, na.rm = TRUE), 
                                    .SDcols = available_vars]
    
    # Calculate proportions for categorical variables
    categorical_summary <- list()
    for (var in c("ST004D01T", "IMMIG")) {
      if (var %in% names(cluster_data)) {
        categorical_summary[[var]] <- prop.table(table(cluster_data[[var]]))
      }
    }
    
    # Count students and resilience rate
    cluster_info <- list(
      n_students = nrow(cluster_data),
      resilience_rate = mean(cluster_data$RESILIENT, na.rm = TRUE),
      numeric_profile = numeric_summary,
      categorical_profile = categorical_summary
    )
    
    cluster_summaries[[as.character(cluster_id)]] <- cluster_info
  }
  
  return(cluster_summaries)
}


detailed_profiles <- list()
for (cycle_name in names(final_clusters)) {
  detailed_profiles[[cycle_name]] <- create_detailed_cluster_profiles(
    cycle_prepared_data[[cycle_name]], 
    final_clusters[[cycle_name]], 
    cycle_name
  )
}# Also fix the radar plot function
```

```{r alternative_visualizations}
# Fix the radar plot function with proper data.table syntax
create_radar_plots <- function(detailed_profiles, cycle_name) {
  library(ggplot2)
  library(scales)
  library(dplyr)
  library(tidyr)
  
  profiles <- detailed_profiles[[cycle_name]]
  
  # Prepare data for radar plots
  radar_data <- data.frame()
  
  for (cluster_id in names(profiles)) {
    cluster_info <- profiles[[cluster_id]]$numeric_profile
    
    # Select key variables for radar plot (standardized)
    key_vars <- c("MATH_AVG", "READ_AVG", "SCIE_AVG", "ESCS", "ANXMAT", "BELONG", "RESILIENT")
    available_vars <- intersect(key_vars, names(cluster_info))
    
    # Extract values using proper data.table syntax
    # Convert to data.frame first to avoid data.table issues
    cluster_info_df <- as.data.frame(cluster_info)
    cluster_values <- as.numeric(cluster_info_df[1, available_vars])
    
    # Standardize values (0-1 scale for radar plot)
    normalized_values <- (cluster_values - min(cluster_values)) / 
      (max(cluster_values) - min(cluster_values))
    
    radar_row <- data.frame(
      cluster = paste("Cluster", cluster_id),
      variable = available_vars,
      value = normalized_values,
      stringsAsFactors = FALSE
    )
    
    radar_data <- rbind(radar_data, radar_row)
  }
  
  # Create radar plot using ggplot2 (since ggradar might not be available)
  p <- ggplot(radar_data, aes(x = variable, y = value, group = cluster, color = cluster)) +
    geom_polygon(fill = NA, alpha = 0.3, size = 1) +
    geom_point(size = 2) +
    coord_polar() +
    theme_minimal() +
    labs(title = paste("PISA", cycle_name, "- Cluster Profiles"),
         subtitle = "Standardized comparison of key variables",
         x = "", y = "") +
    theme(legend.position = "right",
          axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
  
  return(p)
}

# Alternative: Create parallel coordinate plots instead of radar plots
create_parallel_coord_plots <- function(detailed_profiles, cycle_name) {
  library(GGally)
  library(ggplot2)
  
  profiles <- detailed_profiles[[cycle_name]]
  
  # Prepare data for parallel coordinates plot
  plot_data <- data.frame()
  
  for (cluster_id in names(profiles)) {
    cluster_info <- profiles[[cluster_id]]$numeric_profile
    
    # Select key variables
    key_vars <- c("MATH_AVG", "READ_AVG", "SCIE_AVG", "ESCS", "ANXMAT", "BELONG", "RESILIENT")
    available_vars <- intersect(key_vars, names(cluster_info))
    
    # Convert to data.frame and add cluster ID
    cluster_info_df <- as.data.frame(cluster_info)
    cluster_info_df <- cluster_info_df[, available_vars, drop = FALSE]
    cluster_info_df$cluster <- paste("Cluster", cluster_id)
    
    plot_data <- rbind(plot_data, cluster_info_df)
  }
  
  # Create parallel coordinates plot
  p <- ggparcoord(plot_data, 
                  columns = 1:(ncol(plot_data)-1), 
                  groupColumn = "cluster",
                  scale = "uniminmax") +
    theme_minimal() +
    labs(title = paste("PISA", cycle_name, "- Cluster Profiles"),
         x = "Variables", y = "Standardized Values") +
    theme(legend.position = "bottom")
  
  print(p)
  
  return(p)
}

# Create parallel coordinate plots for each cycle (more reliable than radar plots)
parallel_plots <- list()
for (cycle_name in names(detailed_profiles)) {
  parallel_plots[[cycle_name]] <- create_parallel_coord_plots(detailed_profiles, cycle_name)
}

# Create heatmap of cluster characteristics
create_cluster_heatmap <- function(detailed_profiles, cycle_name) {
  library(ggplot2)
  library(reshape2)
  
  profiles <- detailed_profiles[[cycle_name]]
  
  # Prepare data for heatmap
  heatmap_data <- data.frame()
  
  for (cluster_id in names(profiles)) {
    cluster_info <- profiles[[cluster_id]]$numeric_profile
    
    # Select key variables
    key_vars <- c("MATH_AVG", "READ_AVG", "SCIE_AVG", "ESCS", "ANXMAT", "BELONG", "RESILIENT")
    available_vars <- intersect(key_vars, names(cluster_info))
    
    # Convert to data.frame and add cluster ID
    cluster_info_df <- as.data.frame(cluster_info)
    cluster_values <- as.numeric(cluster_info_df[1, available_vars])
    
    for (i in 1:length(available_vars)) {
      heatmap_data <- rbind(heatmap_data, data.frame(
        cluster = paste("Cluster", cluster_id),
        variable = available_vars[i],
        value = cluster_values[i],
        stringsAsFactors = FALSE
      ))
    }
  }
  
  # Create heatmap
  p <- ggplot(heatmap_data, aes(x = variable, y = cluster, fill = value)) +
    geom_tile() +
    scale_fill_viridis_c(option = "plasma") +
    theme_minimal() +
    labs(title = paste("PISA", cycle_name, "- Cluster Characteristics"),
         x = "Variables", y = "Clusters") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
  
  return(p)
}

# Create heatmaps for each cycle
heatmap_plots <- list()
for (cycle_name in names(detailed_profiles)) {
  heatmap_plots[[cycle_name]] <- create_cluster_heatmap(detailed_profiles, cycle_name)
}

# Fix the bar plot creation function to handle missing variables
create_simple_bar_plots <- function(detailed_profiles, cycle_name) {
  library(ggplot2)
  library(gridExtra)
  
  profiles <- detailed_profiles[[cycle_name]]
  
  # Prepare data for bar plots
  plot_data <- data.frame()
  
  for (cluster_id in names(profiles)) {
    cluster_info <- profiles[[cluster_id]]
    
    # Safely extract values with default NA if variable doesn't exist
    math_score <- ifelse("MATH_AVG" %in% names(cluster_info$numeric_profile), 
                         cluster_info$numeric_profile$MATH_AVG, NA)
    reading_score <- ifelse("READ_AVG" %in% names(cluster_info$numeric_profile), 
                            cluster_info$numeric_profile$READ_AVG, NA)
    science_score <- ifelse("SCIE_AVG" %in% names(cluster_info$numeric_profile), 
                            cluster_info$numeric_profile$SCIE_AVG, NA)
    escs <- ifelse("ESCS" %in% names(cluster_info$numeric_profile), 
                   cluster_info$numeric_profile$ESCS, NA)
    anxiety <- ifelse("ANXMAT" %in% names(cluster_info$numeric_profile), 
                      cluster_info$numeric_profile$ANXMAT, NA)
    belonging <- ifelse("BELONG" %in% names(cluster_info$numeric_profile), 
                        cluster_info$numeric_profile$BELONG, NA)
    
    row <- data.frame(
      cluster = paste("Cluster", cluster_id),
      math_score = math_score,
      reading_score = reading_score,
      science_score = science_score,
      escs = escs,
      anxiety = anxiety,
      belonging = belonging,
      resilience_rate = cluster_info$resilience_rate * 100,
      n_students = cluster_info$n_students,
      stringsAsFactors = FALSE
    )
    
    plot_data <- rbind(plot_data, row)
  }
  
  # Create individual bar plots only for available variables
  plots <- list()
  
  if (all(!is.na(plot_data$math_score))) {
    p1 <- ggplot(plot_data, aes(x = cluster, y = math_score, fill = cluster)) +
      geom_bar(stat = "identity") +
      labs(title = paste("PISA", cycle_name, "- Math Achievement"),
           y = "Math Score", x = "") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    plots$math <- p1
  }
  
  if (all(!is.na(plot_data$resilience_rate))) {
    p2 <- ggplot(plot_data, aes(x = cluster, y = resilience_rate, fill = cluster)) +
      geom_bar(stat = "identity") +
      labs(title = paste("PISA", cycle_name, "- Resilience Rate"),
           y = "Resilience Rate (%)", x = "") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    plots$resilience <- p2
  }
  
  if (all(!is.na(plot_data$escs))) {
    p3 <- ggplot(plot_data, aes(x = cluster, y = escs, fill = cluster)) +
      geom_bar(stat = "identity") +
      labs(title = paste("PISA", cycle_name, "- Socioeconomic Status"),
           y = "ESCS", x = "") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    plots$ses <- p3
  }
  
  # Arrange available plots
  if (length(plots) > 0) {
    do.call(grid.arrange, c(plots, ncol = min(3, length(plots))))
  } else {
    cat("No data available for bar plots in", cycle_name, "\n")
  }
  
  return(plots)
}

# Let's also create a function to check what variables are available in each cycle
check_available_variables <- function(detailed_profiles) {
  cat("=== CHECKING AVAILABLE VARIABLES ACROSS CYCLES ===\n")
  
  for (cycle_name in names(detailed_profiles)) {
    cat("\nPISA", cycle_name, "available variables:\n")
    
    # Check the first cluster to see what variables are available
    first_cluster <- detailed_profiles[[cycle_name]][[1]]
    if (!is.null(first_cluster$numeric_profile)) {
      vars <- names(first_cluster$numeric_profile)
      cat("  Numeric variables:", paste(vars, collapse = ", "), "\n")
    }
  }
}

# Check what variables are available
check_available_variables(detailed_profiles)

# Now try creating bar plots again
bar_plots <- list()
for (cycle_name in names(detailed_profiles)) {
  cat("\nCreating bar plots for", cycle_name, "...\n")
  bar_plots[[cycle_name]] <- create_simple_bar_plots(detailed_profiles, cycle_name)
}

# If bar plots still don't work, let's create a simpler table-based visualization
create_cluster_summary_table <- function(detailed_profiles, cycle_name) {
  profiles <- detailed_profiles[[cycle_name]]
  
  # Prepare summary table
  summary_table <- data.frame()
  
  for (cluster_id in names(profiles)) {
    cluster_info <- profiles[[cluster_id]]
    
    # Get all available numeric variables
    numeric_vars <- names(cluster_info$numeric_profile)
    
    # Create a row for this cluster
    row_data <- data.frame(
      Cluster = paste("Cluster", cluster_id),
      Students = cluster_info$n_students,
      Resilience_Rate = round(cluster_info$resilience_rate * 100, 1)
    )
    
    # Add available numeric variables
    for (var in numeric_vars) {
      row_data[[var]] <- round(cluster_info$numeric_profile[[var]], 2)
    }
    
    summary_table <- rbind(summary_table, row_data)
  }
  
  # Print the table
  cat("\n=== PISA", cycle_name, "CLUSTER SUMMARY ===\n")
  print(summary_table)
  
  return(summary_table)
}

# Create summary tables for each cycle
cluster_tables <- list()
for (cycle_name in names(detailed_profiles)) {
  cluster_tables[[cycle_name]] <- create_cluster_summary_table(detailed_profiles, cycle_name)
}

# Create a heatmap visualization from the summary tables
create_heatmap_from_table <- function(cluster_table, cycle_name) {
  library(ggplot2)
  library(reshape2)
  
  # Melt the data for heatmap
  melt_data <- melt(cluster_table, id.vars = c("Cluster", "Students", "Resilience_Rate"))
  
  # Create heatmap
  p <- ggplot(melt_data, aes(x = variable, y = Cluster, fill = value)) +
    geom_tile() +
    scale_fill_viridis_c(option = "plasma") +
    theme_minimal() +
    labs(title = paste("PISA", cycle_name, "- Cluster Characteristics"),
         x = "Variables", y = "Clusters") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
  
  return(p)
}

# Create heatmaps from the summary tables
heatmap_plots <- list()
for (cycle_name in names(cluster_tables)) {
  heatmap_plots[[cycle_name]] <- create_heatmap_from_table(cluster_tables[[cycle_name]], cycle_name)
}

cat("\n✓ Created cluster summary tables and heatmap visualizations\n")
cat("✓ These provide an alternative way to visualize and interpret cluster characteristics\n")
```
